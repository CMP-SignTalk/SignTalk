{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQmKQ7Pobc7L",
    "outputId": "ec4e2d03-cd55-4826-f51c-673dbcf89da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import numpy as np\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "j7h5Rbfubkxw",
    "outputId": "7f2d6f82-c5a1-4665-ff5f-765dc1dd95d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 40, 126)\n",
      "(97,)\n",
      "(48, 40, 126)\n",
      "(48,)\n",
      "(44, 40, 126)\n",
      "(44,)\n",
      "(44, 40, 126)\n",
      "(44,)\n",
      "(44, 40, 126)\n",
      "(44,)\n",
      "(44, 40, 126)\n",
      "(44,)\n",
      "(44, 40, 126)\n",
      "(44,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d2154e767769>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mX_train_temp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/content/drive/MyDrive/Clown_data_40/Data_train_clown_40_no_aug_yes_no_{i}.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mY_train_temp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/content/drive/MyDrive/Clown_data_40/Data_labels_clown_40_no_aug_yes_no_{i}.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mX_train_temp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_train_temp\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0m_ZIP_SUFFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x05\\x06'\u001b[0m \u001b[0;31m# empty zip files start with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "signs = {\"I\":1,\"live\":2,\"love\":3,\"my\":4,\"name\":5,\"sign\":6,\"talk\":7,\"teacher\":8,\"what\":9,\"where\":10,\"yours\":11}\n",
    "\n",
    "\n",
    "X_train=np.load(\"/content/drive/MyDrive/Clown_data_40/Data_train_clown_40_no_aug_yes_no_hello.npy\")\n",
    "Y_train=np.load(\"/content/drive/MyDrive/Clown_data_40/Data_labels_clown_40_no_aug_yes_no_hello.npy\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "X_train=X_train[Y_train==\"hello\"]\n",
    "\n",
    "Y_train=Y_train[Y_train==\"hello\"]\n",
    "\n",
    "for i , k  in zip(signs.keys(),signs.values()):\n",
    "    X_train_temp=np.load(f\"/content/drive/MyDrive/Clown_data_40/Data_train_clown_40_no_aug_yes_no_{i}.npy\")\n",
    "    Y_train_temp=np.load(f\"/content/drive/MyDrive/Clown_data_40/Data_labels_clown_40_no_aug_yes_no_{i}.npy\")\n",
    "\n",
    "    X_train_temp=X_train_temp[Y_train_temp==i]\n",
    "\n",
    "    Y_train_temp=Y_train_temp[Y_train_temp==i]\n",
    "    print(X_train_temp.shape)\n",
    "    print(Y_train_temp.shape)\n",
    "\n",
    "    X_train=np.concatenate([X_train,X_train_temp])\n",
    "    Y_train=np.concatenate([Y_train,Y_train_temp])\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNpsGoxhcrg0"
   },
   "outputs": [],
   "source": [
    "signs = {\"hello\":0,\"I\":1,\"live\":2,\"love\":3,\"my\":4,\"name\":5,\"sign\":6,\"talk\":7,\"teacher\":8,\"what\":9,\"where\":10,\"yours\":11}\n",
    "\n",
    "\n",
    "Y_train_temp=np.zeros(Y_train.shape,dtype=np.int16)\n",
    "for i in range(Y_train.shape[0]):\n",
    "    Y_train_temp[i]=signs[Y_train[i]]\n",
    "\n",
    "Y_train=Y_train_temp\n",
    "\n",
    "del Y_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q7x_MI1dqDR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM , Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DfwXihje-Vk"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(64,return_sequences=True ,activation='relu',input_shape=(40,126)))\n",
    "    model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(signs), activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plxBUorJfx-c"
   },
   "outputs": [],
   "source": [
    "train_shuffler= np.random.permutation(X_train.shape[0])\n",
    "X_train=X_train[train_shuffler]\n",
    "Y_train=Y_train[train_shuffler]\n",
    "\n",
    "\n",
    "Y_train=tf.one_hot(Y_train,depth=len(signs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUJ4veKfCVqb"
   },
   "outputs": [],
   "source": [
    "for item in Y_train:\n",
    "    if( not np.any(item) and np.sum(item)!=1):\n",
    "        print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baYyxfRHCh7B"
   },
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "      for j in range(30):\n",
    "        if(X_train[i][j][X_train[i][j]==None].shape[0] > 0 ):\n",
    "            print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqoOCumOAAK8",
    "outputId": "47351e7d-c694-4dad-f835-e8e49e769cfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(np.argmax(Y_train,axis=1)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmbNS6usf0ch",
    "outputId": "37ab27ba-7e3c-479b-aaa2-5f5f4cea9c23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGa_s2Tgf8oh"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def __init__(self,model_path=\"Iam_model.h5\"):\n",
    "        self.max_val_acc=0\n",
    "        self.max_tra_acc=0\n",
    "        self.model_path=model_path\n",
    "\n",
    "        # old one if the new one give dump results\n",
    "#          if  (val_acc > self.max_val_acc) and \\\n",
    "#             (tra_acc>val_acc or (val_acc>tra_acc and val_acc-tra_acc<=0.01) ):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc=logs.get(\"val_accuracy\")\n",
    "        tra_acc= logs.get(\"accuracy\")\n",
    "        if  (val_acc >= self.max_val_acc or \\\n",
    "             ( tra_acc > self.max_tra_acc  and val_acc >= self.max_val_acc) ) and \\\n",
    "            (tra_acc>val_acc or (val_acc>tra_acc and val_acc-tra_acc<=0.04) ):\n",
    "            print(f\"\\nYes You are here {tra_acc} {val_acc}\")\n",
    "            self.max_val_acc=val_acc\n",
    "            if tra_acc > self.max_tra_acc :\n",
    "                self.max_tra_acc = tra_acc\n",
    "\n",
    "#             with open(self.model_path,\"wb\") as fb:\n",
    "#                 pickle.dump(self.model,fb)\n",
    "            self.model.save(self.model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J5IHO-0dgHAs",
    "outputId": "e52674dd-0dd9-4f56-b69c-47ce1871df19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15000\n",
      "16/16 [==============================] - 7s 255ms/step - loss: 2.4836 - accuracy: 0.0823 - val_loss: 2.4821 - val_accuracy: 0.1273\n",
      "Epoch 2/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4727 - accuracy: 0.1564\n",
      "Yes You are here 0.1563785970211029 0.145454540848732\n",
      "16/16 [==============================] - 3s 155ms/step - loss: 2.4727 - accuracy: 0.1564 - val_loss: 2.4746 - val_accuracy: 0.1455\n",
      "Epoch 3/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4584 - accuracy: 0.1770\n",
      "Yes You are here 0.1769547313451767 0.1818181872367859\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 2.4584 - accuracy: 0.1770 - val_loss: 2.4607 - val_accuracy: 0.1818\n",
      "Epoch 4/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4298 - accuracy: 0.1893\n",
      "Yes You are here 0.18930041790008545 0.1818181872367859\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 2.4298 - accuracy: 0.1893 - val_loss: 2.4292 - val_accuracy: 0.1818\n",
      "Epoch 5/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3734 - accuracy: 0.1934\n",
      "Yes You are here 0.19341564178466797 0.1818181872367859\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 2.3734 - accuracy: 0.1934 - val_loss: 2.4063 - val_accuracy: 0.1818\n",
      "Epoch 6/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3178 - accuracy: 0.2778\n",
      "Yes You are here 0.2777777910232544 0.27272728085517883\n",
      "16/16 [==============================] - 3s 193ms/step - loss: 2.3178 - accuracy: 0.2778 - val_loss: 2.3669 - val_accuracy: 0.2727\n",
      "Epoch 7/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.1979 - accuracy: 0.3169\n",
      "Yes You are here 0.31687241792678833 0.27272728085517883\n",
      "16/16 [==============================] - 3s 202ms/step - loss: 2.1979 - accuracy: 0.3169 - val_loss: 3.2201 - val_accuracy: 0.2727\n",
      "Epoch 8/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2972 - accuracy: 0.3416\n",
      "Yes You are here 0.34156379103660583 0.290909081697464\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 2.2972 - accuracy: 0.3416 - val_loss: 2.2576 - val_accuracy: 0.2909\n",
      "Epoch 9/15000\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 2.0988 - accuracy: 0.3745 - val_loss: 2.2092 - val_accuracy: 0.2727\n",
      "Epoch 10/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.0092 - accuracy: 0.3930\n",
      "Yes You are here 0.3930041193962097 0.290909081697464\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 2.0092 - accuracy: 0.3930 - val_loss: 2.1184 - val_accuracy: 0.2909\n",
      "Epoch 11/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.8875 - accuracy: 0.4074\n",
      "Yes You are here 0.40740740299224854 0.30909091234207153\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.8875 - accuracy: 0.4074 - val_loss: 1.9959 - val_accuracy: 0.3091\n",
      "Epoch 12/15000\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 1.8475 - accuracy: 0.4733 - val_loss: 2.8874 - val_accuracy: 0.2909\n",
      "Epoch 13/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.7539 - accuracy: 0.4300\n",
      "Yes You are here 0.4300411641597748 0.38181817531585693\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 3.7539 - accuracy: 0.4300 - val_loss: 3.9442 - val_accuracy: 0.3818\n",
      "Epoch 14/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 5.6810 - accuracy: 0.4342 - val_loss: 9.7431 - val_accuracy: 0.2545\n",
      "Epoch 15/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.9812 - accuracy: 0.4486\n",
      "Yes You are here 0.4485596716403961 0.4000000059604645\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 4.9812 - accuracy: 0.4486 - val_loss: 2.2494 - val_accuracy: 0.4000\n",
      "Epoch 16/15000\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 2.4751 - accuracy: 0.4712 - val_loss: 2.0136 - val_accuracy: 0.3818\n",
      "Epoch 17/15000\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 2.0010 - accuracy: 0.4630 - val_loss: 2.0145 - val_accuracy: 0.3455\n",
      "Epoch 18/15000\n",
      "16/16 [==============================] - 3s 204ms/step - loss: 1.8627 - accuracy: 0.4568 - val_loss: 2.0137 - val_accuracy: 0.3455\n",
      "Epoch 19/15000\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 1.8098 - accuracy: 0.4588 - val_loss: 1.9908 - val_accuracy: 0.3636\n",
      "Epoch 20/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7840 - accuracy: 0.4650\n",
      "Yes You are here 0.4650205671787262 0.4000000059604645\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.7840 - accuracy: 0.4650 - val_loss: 1.9819 - val_accuracy: 0.4000\n",
      "Epoch 21/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7592 - accuracy: 0.4691\n",
      "Yes You are here 0.4691357910633087 0.41818180680274963\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 1.7592 - accuracy: 0.4691 - val_loss: 1.9741 - val_accuracy: 0.4182\n",
      "Epoch 22/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.7590 - accuracy: 0.4630 - val_loss: 1.9645 - val_accuracy: 0.3818\n",
      "Epoch 23/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7595 - accuracy: 0.4794\n",
      "Yes You are here 0.4794238805770874 0.41818180680274963\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 1.7595 - accuracy: 0.4794 - val_loss: 1.9570 - val_accuracy: 0.4182\n",
      "Epoch 24/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7081 - accuracy: 0.4877\n",
      "Yes You are here 0.48765432834625244 0.4545454680919647\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 1.7081 - accuracy: 0.4877 - val_loss: 1.9501 - val_accuracy: 0.4545\n",
      "Epoch 25/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.7380 - accuracy: 0.4856\n",
      "Yes You are here 0.4855967164039612 0.4545454680919647\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 1.7380 - accuracy: 0.4856 - val_loss: 1.9418 - val_accuracy: 0.4545\n",
      "Epoch 26/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.6868 - accuracy: 0.5041\n",
      "Yes You are here 0.5041152238845825 0.4727272689342499\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.6868 - accuracy: 0.5041 - val_loss: 1.9344 - val_accuracy: 0.4727\n",
      "Epoch 27/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.6571 - accuracy: 0.5062\n",
      "Yes You are here 0.5061728358268738 0.4727272689342499\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.6571 - accuracy: 0.5062 - val_loss: 1.9255 - val_accuracy: 0.4727\n",
      "Epoch 28/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.6395 - accuracy: 0.5206\n",
      "Yes You are here 0.5205761194229126 0.4727272689342499\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 1.6395 - accuracy: 0.5206 - val_loss: 1.9159 - val_accuracy: 0.4727\n",
      "Epoch 29/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.6318 - accuracy: 0.5309\n",
      "Yes You are here 0.5308641791343689 0.4909090995788574\n",
      "16/16 [==============================] - 3s 210ms/step - loss: 1.6318 - accuracy: 0.5309 - val_loss: 1.9082 - val_accuracy: 0.4909\n",
      "Epoch 30/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 1.6194 - accuracy: 0.5432 - val_loss: 1.9008 - val_accuracy: 0.4727\n",
      "Epoch 31/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.6502 - accuracy: 0.5370 - val_loss: 1.8957 - val_accuracy: 0.4545\n",
      "Epoch 32/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.6059 - accuracy: 0.5473 - val_loss: 1.8878 - val_accuracy: 0.4545\n",
      "Epoch 33/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 1.5974 - accuracy: 0.5370 - val_loss: 1.8793 - val_accuracy: 0.4545\n",
      "Epoch 34/15000\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 1.5814 - accuracy: 0.5453 - val_loss: 1.8723 - val_accuracy: 0.4727\n",
      "Epoch 35/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.5714 - accuracy: 0.5597 - val_loss: 1.8640 - val_accuracy: 0.4727\n",
      "Epoch 36/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.5624 - accuracy: 0.5720\n",
      "Yes You are here 0.5720164775848389 0.5090909004211426\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 1.5624 - accuracy: 0.5720 - val_loss: 1.8540 - val_accuracy: 0.5091\n",
      "Epoch 37/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.5551 - accuracy: 0.5741\n",
      "Yes You are here 0.5740740895271301 0.5454545617103577\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.5551 - accuracy: 0.5741 - val_loss: 1.8483 - val_accuracy: 0.5455\n",
      "Epoch 38/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 1.5449 - accuracy: 0.5761 - val_loss: 1.8335 - val_accuracy: 0.5273\n",
      "Epoch 39/15000\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 1.5377 - accuracy: 0.5823 - val_loss: 1.8269 - val_accuracy: 0.5273\n",
      "Epoch 40/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 1.5286 - accuracy: 0.5864 - val_loss: 1.8099 - val_accuracy: 0.5273\n",
      "Epoch 41/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.5173 - accuracy: 0.5885 - val_loss: 1.8035 - val_accuracy: 0.5273\n",
      "Epoch 42/15000\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1.5066 - accuracy: 0.5926 - val_loss: 1.7824 - val_accuracy: 0.5273\n",
      "Epoch 43/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.4959 - accuracy: 0.5864 - val_loss: 1.7749 - val_accuracy: 0.5273\n",
      "Epoch 44/15000\n",
      "16/16 [==============================] - 3s 195ms/step - loss: 1.4851 - accuracy: 0.5905 - val_loss: 1.7546 - val_accuracy: 0.5273\n",
      "Epoch 45/15000\n",
      "16/16 [==============================] - 3s 196ms/step - loss: 2.1129 - accuracy: 0.5700 - val_loss: 1.7712 - val_accuracy: 0.4727\n",
      "Epoch 46/15000\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 1.5171 - accuracy: 0.5535 - val_loss: 1.7725 - val_accuracy: 0.4727\n",
      "Epoch 47/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.4847 - accuracy: 0.5802 - val_loss: 1.7458 - val_accuracy: 0.4909\n",
      "Epoch 48/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.4674 - accuracy: 0.5864 - val_loss: 1.7229 - val_accuracy: 0.5091\n",
      "Epoch 49/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.4739 - accuracy: 0.5947 - val_loss: 1.7088 - val_accuracy: 0.5091\n",
      "Epoch 50/15000\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 1.4455 - accuracy: 0.6132 - val_loss: 1.6934 - val_accuracy: 0.5273\n",
      "Epoch 51/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.4316 - accuracy: 0.6193 - val_loss: 1.6786 - val_accuracy: 0.5273\n",
      "Epoch 52/15000\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 1.4266 - accuracy: 0.6193 - val_loss: 1.6691 - val_accuracy: 0.5273\n",
      "Epoch 53/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.4150 - accuracy: 0.6152\n",
      "Yes You are here 0.6152263283729553 0.581818163394928\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.4150 - accuracy: 0.6152 - val_loss: 1.6563 - val_accuracy: 0.5818\n",
      "Epoch 54/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.4191 - accuracy: 0.6070\n",
      "Yes You are here 0.6069958806037903 0.581818163394928\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 1.4191 - accuracy: 0.6070 - val_loss: 1.6488 - val_accuracy: 0.5818\n",
      "Epoch 55/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.4266 - accuracy: 0.6091\n",
      "Yes You are here 0.6090534925460815 0.581818163394928\n",
      "16/16 [==============================] - 3s 202ms/step - loss: 1.4266 - accuracy: 0.6091 - val_loss: 1.6365 - val_accuracy: 0.5818\n",
      "Epoch 56/15000\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 6.1847 - accuracy: 0.5700 - val_loss: 4.0071 - val_accuracy: 0.4364\n",
      "Epoch 57/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 9.7678 - accuracy: 0.5432 - val_loss: 6.2767 - val_accuracy: 0.4364\n",
      "Epoch 58/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 7.1715 - accuracy: 0.5412 - val_loss: 1.7424 - val_accuracy: 0.3818\n",
      "Epoch 59/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 1.7150 - accuracy: 0.5206 - val_loss: 1.7937 - val_accuracy: 0.3636\n",
      "Epoch 60/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.5466 - accuracy: 0.5267 - val_loss: 1.7729 - val_accuracy: 0.3636\n",
      "Epoch 61/15000\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 1.5171 - accuracy: 0.5370 - val_loss: 1.7442 - val_accuracy: 0.4182\n",
      "Epoch 62/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 1.4914 - accuracy: 0.5556 - val_loss: 1.7325 - val_accuracy: 0.4545\n",
      "Epoch 63/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.4787 - accuracy: 0.5761 - val_loss: 1.7219 - val_accuracy: 0.5091\n",
      "Epoch 64/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.4653 - accuracy: 0.5926 - val_loss: 1.7159 - val_accuracy: 0.5091\n",
      "Epoch 65/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.4605 - accuracy: 0.5967 - val_loss: 1.7064 - val_accuracy: 0.5091\n",
      "Epoch 66/15000\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 1.4540 - accuracy: 0.6008 - val_loss: 1.6958 - val_accuracy: 0.5091\n",
      "Epoch 67/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.4512 - accuracy: 0.6029 - val_loss: 1.6876 - val_accuracy: 0.5273\n",
      "Epoch 68/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.4402 - accuracy: 0.6070 - val_loss: 1.6823 - val_accuracy: 0.5455\n",
      "Epoch 69/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.4311 - accuracy: 0.6132 - val_loss: 1.6724 - val_accuracy: 0.5273\n",
      "Epoch 70/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.4237 - accuracy: 0.6214 - val_loss: 1.6740 - val_accuracy: 0.5455\n",
      "Epoch 71/15000\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 1.4192 - accuracy: 0.6173 - val_loss: 1.6660 - val_accuracy: 0.5091\n",
      "Epoch 72/15000\n",
      "16/16 [==============================] - 3s 205ms/step - loss: 1.4188 - accuracy: 0.6173 - val_loss: 1.6528 - val_accuracy: 0.5455\n",
      "Epoch 73/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.4087 - accuracy: 0.6276 - val_loss: 1.6452 - val_accuracy: 0.5455\n",
      "Epoch 74/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.4043 - accuracy: 0.6255 - val_loss: 1.6378 - val_accuracy: 0.5455\n",
      "Epoch 75/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.3979 - accuracy: 0.6317 - val_loss: 1.6350 - val_accuracy: 0.5455\n",
      "Epoch 76/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.3936 - accuracy: 0.6317 - val_loss: 1.6288 - val_accuracy: 0.5455\n",
      "Epoch 77/15000\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 1.3908 - accuracy: 0.6296 - val_loss: 1.6288 - val_accuracy: 0.5455\n",
      "Epoch 78/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.3896 - accuracy: 0.6358 - val_loss: 1.6169 - val_accuracy: 0.5455\n",
      "Epoch 79/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.3791 - accuracy: 0.6379 - val_loss: 1.6167 - val_accuracy: 0.5455\n",
      "Epoch 80/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.3754 - accuracy: 0.6399 - val_loss: 1.6053 - val_accuracy: 0.5273\n",
      "Epoch 81/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.3693 - accuracy: 0.6420 - val_loss: 1.6023 - val_accuracy: 0.5273\n",
      "Epoch 82/15000\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 1.3675 - accuracy: 0.6420 - val_loss: 1.5961 - val_accuracy: 0.5273\n",
      "Epoch 83/15000\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 1.3631 - accuracy: 0.6420 - val_loss: 1.5899 - val_accuracy: 0.5273\n",
      "Epoch 84/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.3589 - accuracy: 0.6358 - val_loss: 1.5862 - val_accuracy: 0.5273\n",
      "Epoch 85/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.3532 - accuracy: 0.6461 - val_loss: 1.5823 - val_accuracy: 0.5273\n",
      "Epoch 86/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.3488 - accuracy: 0.6502 - val_loss: 1.5766 - val_accuracy: 0.5273\n",
      "Epoch 87/15000\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 1.3454 - accuracy: 0.6481 - val_loss: 1.5722 - val_accuracy: 0.5273\n",
      "Epoch 88/15000\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 1.3394 - accuracy: 0.6543 - val_loss: 1.5688 - val_accuracy: 0.5273\n",
      "Epoch 89/15000\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1.3370 - accuracy: 0.6502 - val_loss: 1.5612 - val_accuracy: 0.5273\n",
      "Epoch 90/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.3356 - accuracy: 0.6543 - val_loss: 1.5604 - val_accuracy: 0.5091\n",
      "Epoch 91/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.3274 - accuracy: 0.6564 - val_loss: 1.5666 - val_accuracy: 0.4909\n",
      "Epoch 92/15000\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1.3287 - accuracy: 0.6543 - val_loss: 1.5495 - val_accuracy: 0.5455\n",
      "Epoch 93/15000\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 1.3212 - accuracy: 0.6584 - val_loss: 1.5442 - val_accuracy: 0.5455\n",
      "Epoch 94/15000\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 1.3171 - accuracy: 0.6564 - val_loss: 1.5444 - val_accuracy: 0.5636\n",
      "Epoch 95/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.3100 - accuracy: 0.6584 - val_loss: 1.5349 - val_accuracy: 0.5455\n",
      "Epoch 96/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.3068 - accuracy: 0.6626 - val_loss: 1.5315 - val_accuracy: 0.5636\n",
      "Epoch 97/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.3029 - accuracy: 0.6605 - val_loss: 1.5299 - val_accuracy: 0.5636\n",
      "Epoch 98/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.3203 - accuracy: 0.6543 - val_loss: 1.5321 - val_accuracy: 0.5636\n",
      "Epoch 99/15000\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 1.3035 - accuracy: 0.6564 - val_loss: 1.5558 - val_accuracy: 0.5455\n",
      "Epoch 100/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.3055 - accuracy: 0.6481 - val_loss: 1.5215 - val_accuracy: 0.5636\n",
      "Epoch 101/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.2945 - accuracy: 0.6605 - val_loss: 1.5174 - val_accuracy: 0.5636\n",
      "Epoch 102/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.2804 - accuracy: 0.6584 - val_loss: 1.5104 - val_accuracy: 0.5455\n",
      "Epoch 103/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 1.2863 - accuracy: 0.6584 - val_loss: 1.5400 - val_accuracy: 0.5273\n",
      "Epoch 104/15000\n",
      "16/16 [==============================] - 3s 219ms/step - loss: 1.2867 - accuracy: 0.6523 - val_loss: 1.5140 - val_accuracy: 0.5455\n",
      "Epoch 105/15000\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 1.3448 - accuracy: 0.6440 - val_loss: 1.5292 - val_accuracy: 0.5273\n",
      "Epoch 106/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 1.2787 - accuracy: 0.6523 - val_loss: 1.4994 - val_accuracy: 0.5636\n",
      "Epoch 107/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.3032 - accuracy: 0.6564 - val_loss: 1.4960 - val_accuracy: 0.5636\n",
      "Epoch 108/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.2722 - accuracy: 0.6564 - val_loss: 1.4992 - val_accuracy: 0.5455\n",
      "Epoch 109/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 1.2558 - accuracy: 0.6646 - val_loss: 1.4875 - val_accuracy: 0.5455\n",
      "Epoch 110/15000\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 1.2512 - accuracy: 0.6626 - val_loss: 1.4890 - val_accuracy: 0.5455\n",
      "Epoch 111/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.2465 - accuracy: 0.6605 - val_loss: 1.4897 - val_accuracy: 0.5455\n",
      "Epoch 112/15000\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1.2414 - accuracy: 0.6605 - val_loss: 1.4791 - val_accuracy: 0.5455\n",
      "Epoch 113/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.2482 - accuracy: 0.6584 - val_loss: 1.5037 - val_accuracy: 0.5455\n",
      "Epoch 114/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.2447 - accuracy: 0.6543 - val_loss: 1.4841 - val_accuracy: 0.5273\n",
      "Epoch 115/15000\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 1.2784 - accuracy: 0.6523 - val_loss: 1.7421 - val_accuracy: 0.5273\n",
      "Epoch 116/15000\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 1.3263 - accuracy: 0.6420 - val_loss: 1.4650 - val_accuracy: 0.5636\n",
      "Epoch 117/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 1.2378 - accuracy: 0.6502 - val_loss: 1.4647 - val_accuracy: 0.5636\n",
      "Epoch 118/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.2231 - accuracy: 0.6584 - val_loss: 1.4730 - val_accuracy: 0.5273\n",
      "Epoch 119/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.2145 - accuracy: 0.6605 - val_loss: 1.4534 - val_accuracy: 0.5455\n",
      "Epoch 120/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.2090 - accuracy: 0.6646 - val_loss: 1.4645 - val_accuracy: 0.5273\n",
      "Epoch 121/15000\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 1.2147 - accuracy: 0.6584 - val_loss: 1.4539 - val_accuracy: 0.5455\n",
      "Epoch 122/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.2103 - accuracy: 0.6626 - val_loss: 1.4575 - val_accuracy: 0.5455\n",
      "Epoch 123/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 1.2070 - accuracy: 0.6646 - val_loss: 1.4591 - val_accuracy: 0.5455\n",
      "Epoch 124/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.2030 - accuracy: 0.6626 - val_loss: 1.4453 - val_accuracy: 0.5455\n",
      "Epoch 125/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 1.2658 - accuracy: 0.6502 - val_loss: 1.5105 - val_accuracy: 0.5273\n",
      "Epoch 126/15000\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 1.2622 - accuracy: 0.6461 - val_loss: 1.4334 - val_accuracy: 0.5636\n",
      "Epoch 127/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 1.2574 - accuracy: 0.6502 - val_loss: 1.4416 - val_accuracy: 0.5455\n",
      "Epoch 128/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.1944 - accuracy: 0.6584 - val_loss: 1.4315 - val_accuracy: 0.5455\n",
      "Epoch 129/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.1853 - accuracy: 0.6605 - val_loss: 1.4253 - val_accuracy: 0.5455\n",
      "Epoch 130/15000\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1.1784 - accuracy: 0.6626 - val_loss: 1.4262 - val_accuracy: 0.5455\n",
      "Epoch 131/15000\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 1.1762 - accuracy: 0.6646 - val_loss: 1.4232 - val_accuracy: 0.5455\n",
      "Epoch 132/15000\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 1.1749 - accuracy: 0.6605 - val_loss: 1.4183 - val_accuracy: 0.5455\n",
      "Epoch 133/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.1976 - accuracy: 0.6584 - val_loss: 1.4146 - val_accuracy: 0.5455\n",
      "Epoch 134/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.1601 - accuracy: 0.6687 - val_loss: 1.4320 - val_accuracy: 0.5273\n",
      "Epoch 135/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.1545 - accuracy: 0.6708 - val_loss: 1.4150 - val_accuracy: 0.5455\n",
      "Epoch 136/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.1501 - accuracy: 0.6728 - val_loss: 1.4242 - val_accuracy: 0.5273\n",
      "Epoch 137/15000\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 1.1518 - accuracy: 0.6667 - val_loss: 1.4102 - val_accuracy: 0.5455\n",
      "Epoch 138/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 1.1424 - accuracy: 0.6728 - val_loss: 1.4222 - val_accuracy: 0.5273\n",
      "Epoch 139/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.1387 - accuracy: 0.6708 - val_loss: 1.4040 - val_accuracy: 0.5455\n",
      "Epoch 140/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.1330 - accuracy: 0.6646 - val_loss: 1.4126 - val_accuracy: 0.5273\n",
      "Epoch 141/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.1297 - accuracy: 0.6667 - val_loss: 1.3935 - val_accuracy: 0.5455\n",
      "Epoch 142/15000\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 1.1266 - accuracy: 0.6667 - val_loss: 1.4015 - val_accuracy: 0.5273\n",
      "Epoch 143/15000\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 1.1228 - accuracy: 0.6687 - val_loss: 1.3933 - val_accuracy: 0.5273\n",
      "Epoch 144/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.1170 - accuracy: 0.6708 - val_loss: 1.3938 - val_accuracy: 0.5273\n",
      "Epoch 145/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.1133 - accuracy: 0.6687 - val_loss: 1.3820 - val_accuracy: 0.5455\n",
      "Epoch 146/15000\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1.1098 - accuracy: 0.6667 - val_loss: 1.3814 - val_accuracy: 0.5455\n",
      "Epoch 147/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.2005 - accuracy: 0.6667 - val_loss: 1.4022 - val_accuracy: 0.5273\n",
      "Epoch 148/15000\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 1.1434 - accuracy: 0.6564 - val_loss: 1.4070 - val_accuracy: 0.5273\n",
      "Epoch 149/15000\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 1.1387 - accuracy: 0.6523 - val_loss: 1.3984 - val_accuracy: 0.5273\n",
      "Epoch 150/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.1252 - accuracy: 0.6626 - val_loss: 1.4283 - val_accuracy: 0.5273\n",
      "Epoch 151/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.1193 - accuracy: 0.6667 - val_loss: 1.3926 - val_accuracy: 0.5273\n",
      "Epoch 152/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.1171 - accuracy: 0.6687 - val_loss: 1.3903 - val_accuracy: 0.5455\n",
      "Epoch 153/15000\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 1.1141 - accuracy: 0.6646 - val_loss: 1.4184 - val_accuracy: 0.5273\n",
      "Epoch 154/15000\n",
      "16/16 [==============================] - 4s 216ms/step - loss: 1.1082 - accuracy: 0.6646 - val_loss: 1.3807 - val_accuracy: 0.5455\n",
      "Epoch 155/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.1048 - accuracy: 0.6687 - val_loss: 1.3826 - val_accuracy: 0.5455\n",
      "Epoch 156/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 1.1021 - accuracy: 0.6646 - val_loss: 1.3743 - val_accuracy: 0.5455\n",
      "Epoch 157/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.0983 - accuracy: 0.6646 - val_loss: 1.3639 - val_accuracy: 0.5636\n",
      "Epoch 158/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.0913 - accuracy: 0.6728 - val_loss: 1.3763 - val_accuracy: 0.5455\n",
      "Epoch 159/15000\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 1.0875 - accuracy: 0.6687 - val_loss: 1.3743 - val_accuracy: 0.5455\n",
      "Epoch 160/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.0837 - accuracy: 0.6708 - val_loss: 1.3774 - val_accuracy: 0.5455\n",
      "Epoch 161/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.0818 - accuracy: 0.6708 - val_loss: 1.3448 - val_accuracy: 0.5636\n",
      "Epoch 162/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.0752 - accuracy: 0.6728 - val_loss: 1.4064 - val_accuracy: 0.5636\n",
      "Epoch 163/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.0771 - accuracy: 0.6728 - val_loss: 1.3451 - val_accuracy: 0.5455\n",
      "Epoch 164/15000\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 1.1083 - accuracy: 0.6626 - val_loss: 1.5569 - val_accuracy: 0.5455\n",
      "Epoch 165/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1431 - accuracy: 0.6502\n",
      "Yes You are here 0.6502057909965515 0.581818163394928\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 1.1431 - accuracy: 0.6502 - val_loss: 1.3252 - val_accuracy: 0.5818\n",
      "Epoch 166/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.1134 - accuracy: 0.6564 - val_loss: 1.3499 - val_accuracy: 0.5455\n",
      "Epoch 167/15000\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1.0833 - accuracy: 0.6605 - val_loss: 1.3222 - val_accuracy: 0.5636\n",
      "Epoch 168/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.0611 - accuracy: 0.6708 - val_loss: 1.3149 - val_accuracy: 0.5636\n",
      "Epoch 169/15000\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1.0899 - accuracy: 0.6667 - val_loss: 1.8280 - val_accuracy: 0.5273\n",
      "Epoch 170/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1900 - accuracy: 0.6502\n",
      "Yes You are here 0.6502057909965515 0.581818163394928\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 1.1900 - accuracy: 0.6502 - val_loss: 1.3084 - val_accuracy: 0.5818\n",
      "Epoch 171/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2357 - accuracy: 0.6523\n",
      "Yes You are here 0.6522634029388428 0.581818163394928\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.2357 - accuracy: 0.6523 - val_loss: 1.3055 - val_accuracy: 0.5818\n",
      "Epoch 172/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.0502 - accuracy: 0.6605 - val_loss: 1.3871 - val_accuracy: 0.5455\n",
      "Epoch 173/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 1.0779 - accuracy: 0.6564 - val_loss: 1.3028 - val_accuracy: 0.5636\n",
      "Epoch 174/15000\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1.0441 - accuracy: 0.6687 - val_loss: 1.2962 - val_accuracy: 0.5636\n",
      "Epoch 175/15000\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 1.0382 - accuracy: 0.6728 - val_loss: 1.3035 - val_accuracy: 0.5455\n",
      "Epoch 176/15000\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 1.0473 - accuracy: 0.6728 - val_loss: 1.3377 - val_accuracy: 0.5273\n",
      "Epoch 177/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.0264 - accuracy: 0.6749 - val_loss: 1.2984 - val_accuracy: 0.5455\n",
      "Epoch 178/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.0204 - accuracy: 0.6770 - val_loss: 1.3816 - val_accuracy: 0.5273\n",
      "Epoch 179/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.0209 - accuracy: 0.6749 - val_loss: 1.2830 - val_accuracy: 0.5455\n",
      "Epoch 180/15000\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 2.4825 - accuracy: 0.6173 - val_loss: 2.0528 - val_accuracy: 0.5455\n",
      "Epoch 181/15000\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 1.6704 - accuracy: 0.6173 - val_loss: 1.6655 - val_accuracy: 0.5273\n",
      "Epoch 182/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.3808 - accuracy: 0.6235 - val_loss: 1.3876 - val_accuracy: 0.5455\n",
      "Epoch 183/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1881 - accuracy: 0.6317\n",
      "Yes You are here 0.6316872239112854 0.581818163394928\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.1881 - accuracy: 0.6317 - val_loss: 1.6199 - val_accuracy: 0.5818\n",
      "Epoch 184/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1.1517 - accuracy: 0.6379 - val_loss: 1.4490 - val_accuracy: 0.5455\n",
      "Epoch 185/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.1242 - accuracy: 0.6502 - val_loss: 1.5366 - val_accuracy: 0.5636\n",
      "Epoch 186/15000\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 1.0960 - accuracy: 0.6626 - val_loss: 1.4650 - val_accuracy: 0.5455\n",
      "Epoch 187/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 1.0819 - accuracy: 0.6605 - val_loss: 1.5146 - val_accuracy: 0.5636\n",
      "Epoch 188/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.0771 - accuracy: 0.6667 - val_loss: 1.4605 - val_accuracy: 0.5636\n",
      "Epoch 189/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1.0653 - accuracy: 0.6605 - val_loss: 1.4026 - val_accuracy: 0.5455\n",
      "Epoch 190/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0856 - accuracy: 0.6605\n",
      "Yes You are here 0.6604938507080078 0.581818163394928\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.0856 - accuracy: 0.6605 - val_loss: 1.4299 - val_accuracy: 0.5818\n",
      "Epoch 191/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0507 - accuracy: 0.6605\n",
      "Yes You are here 0.6604938507080078 0.581818163394928\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 1.0507 - accuracy: 0.6605 - val_loss: 1.4386 - val_accuracy: 0.5818\n",
      "Epoch 192/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0452 - accuracy: 0.6728\n",
      "Yes You are here 0.6728395223617554 0.581818163394928\n",
      "16/16 [==============================] - 4s 219ms/step - loss: 1.0452 - accuracy: 0.6728 - val_loss: 1.4350 - val_accuracy: 0.5818\n",
      "Epoch 193/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0375 - accuracy: 0.6811\n",
      "Yes You are here 0.6810699701309204 0.581818163394928\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.0375 - accuracy: 0.6811 - val_loss: 1.4112 - val_accuracy: 0.5818\n",
      "Epoch 194/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.6852\n",
      "Yes You are here 0.6851851940155029 0.581818163394928\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 1.0344 - accuracy: 0.6852 - val_loss: 1.4121 - val_accuracy: 0.5818\n",
      "Epoch 195/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.0284 - accuracy: 0.6749 - val_loss: 1.4307 - val_accuracy: 0.5636\n",
      "Epoch 196/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1.0284 - accuracy: 0.6728 - val_loss: 1.4129 - val_accuracy: 0.5636\n",
      "Epoch 197/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0185 - accuracy: 0.6872\n",
      "Yes You are here 0.6872428059577942 0.581818163394928\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 1.0185 - accuracy: 0.6872 - val_loss: 1.3859 - val_accuracy: 0.5818\n",
      "Epoch 198/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0164 - accuracy: 0.6893\n",
      "Yes You are here 0.6893004179000854 0.581818163394928\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 1.0164 - accuracy: 0.6893 - val_loss: 1.3700 - val_accuracy: 0.5818\n",
      "Epoch 199/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1.0365 - accuracy: 0.6934 - val_loss: 1.2817 - val_accuracy: 0.5636\n",
      "Epoch 200/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1.0134 - accuracy: 0.6852 - val_loss: 1.4590 - val_accuracy: 0.5636\n",
      "Epoch 201/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0114 - accuracy: 0.6852\n",
      "Yes You are here 0.6851851940155029 0.581818163394928\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 1.0114 - accuracy: 0.6852 - val_loss: 1.3517 - val_accuracy: 0.5818\n",
      "Epoch 202/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0065 - accuracy: 0.6872\n",
      "Yes You are here 0.6872428059577942 0.581818163394928\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 1.0065 - accuracy: 0.6872 - val_loss: 1.4256 - val_accuracy: 0.5818\n",
      "Epoch 203/15000\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.9935 - accuracy: 0.6872 - val_loss: 1.3787 - val_accuracy: 0.5455\n",
      "Epoch 204/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.9911 - accuracy: 0.6831 - val_loss: 1.4337 - val_accuracy: 0.5636\n",
      "Epoch 205/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.9918 - accuracy: 0.6811 - val_loss: 1.3262 - val_accuracy: 0.5636\n",
      "Epoch 206/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.9938 - accuracy: 0.6811 - val_loss: 1.2463 - val_accuracy: 0.5636\n",
      "Epoch 207/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.9823 - accuracy: 0.6893 - val_loss: 1.2639 - val_accuracy: 0.5636\n",
      "Epoch 208/15000\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.9708 - accuracy: 0.6975 - val_loss: 1.2586 - val_accuracy: 0.5636\n",
      "Epoch 209/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.9684 - accuracy: 0.6996 - val_loss: 1.2607 - val_accuracy: 0.5636\n",
      "Epoch 210/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.9640 - accuracy: 0.6914 - val_loss: 1.2942 - val_accuracy: 0.5636\n",
      "Epoch 211/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.9572 - accuracy: 0.7037 - val_loss: 1.2657 - val_accuracy: 0.5636\n",
      "Epoch 212/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9527 - accuracy: 0.6975\n",
      "Yes You are here 0.6975308656692505 0.581818163394928\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.9527 - accuracy: 0.6975 - val_loss: 1.2614 - val_accuracy: 0.5818\n",
      "Epoch 213/15000\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.9487 - accuracy: 0.6975 - val_loss: 1.2654 - val_accuracy: 0.5636\n",
      "Epoch 214/15000\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 0.9434 - accuracy: 0.6975 - val_loss: 1.2529 - val_accuracy: 0.5636\n",
      "Epoch 215/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 0.9405 - accuracy: 0.6996 - val_loss: 1.2188 - val_accuracy: 0.5636\n",
      "Epoch 216/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 0.9369 - accuracy: 0.6955 - val_loss: 1.2274 - val_accuracy: 0.5636\n",
      "Epoch 217/15000\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 0.9358 - accuracy: 0.6955 - val_loss: 1.2327 - val_accuracy: 0.5636\n",
      "Epoch 218/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.9288 - accuracy: 0.6914 - val_loss: 1.2336 - val_accuracy: 0.5636\n",
      "Epoch 219/15000\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.9303 - accuracy: 0.6975 - val_loss: 1.1795 - val_accuracy: 0.5636\n",
      "Epoch 220/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.9305 - accuracy: 0.6955 - val_loss: 1.3070 - val_accuracy: 0.5636\n",
      "Epoch 221/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.9146 - accuracy: 0.6934 - val_loss: 1.1909 - val_accuracy: 0.5455\n",
      "Epoch 222/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.9177 - accuracy: 0.6914 - val_loss: 1.2252 - val_accuracy: 0.5636\n",
      "Epoch 223/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.9081 - accuracy: 0.6975 - val_loss: 1.2007 - val_accuracy: 0.5636\n",
      "Epoch 224/15000\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.9065 - accuracy: 0.6934 - val_loss: 1.2160 - val_accuracy: 0.5636\n",
      "Epoch 225/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.8976 - accuracy: 0.6975 - val_loss: 1.2033 - val_accuracy: 0.5636\n",
      "Epoch 226/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.8933 - accuracy: 0.6975 - val_loss: 1.1796 - val_accuracy: 0.5636\n",
      "Epoch 227/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.8900 - accuracy: 0.7099 - val_loss: 1.1673 - val_accuracy: 0.5636\n",
      "Epoch 228/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.8934 - accuracy: 0.7058 - val_loss: 1.2279 - val_accuracy: 0.5636\n",
      "Epoch 229/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8861 - accuracy: 0.7099\n",
      "Yes You are here 0.709876537322998 0.581818163394928\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.8861 - accuracy: 0.7099 - val_loss: 1.1441 - val_accuracy: 0.5818\n",
      "Epoch 230/15000\n",
      "16/16 [==============================] - 3s 195ms/step - loss: 0.8773 - accuracy: 0.7078 - val_loss: 1.1362 - val_accuracy: 0.5636\n",
      "Epoch 231/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8734 - accuracy: 0.7058\n",
      "Yes You are here 0.7057613134384155 0.581818163394928\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.8734 - accuracy: 0.7058 - val_loss: 1.1333 - val_accuracy: 0.5818\n",
      "Epoch 232/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.8688 - accuracy: 0.7016 - val_loss: 1.1341 - val_accuracy: 0.5636\n",
      "Epoch 233/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.8694 - accuracy: 0.7016 - val_loss: 1.1167 - val_accuracy: 0.5636\n",
      "Epoch 234/15000\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.9286 - accuracy: 0.6955 - val_loss: 1.1450 - val_accuracy: 0.5636\n",
      "Epoch 235/15000\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.8898 - accuracy: 0.6831 - val_loss: 1.1631 - val_accuracy: 0.5636\n",
      "Epoch 236/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8575 - accuracy: 0.7037\n",
      "Yes You are here 0.7037037014961243 0.581818163394928\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.8575 - accuracy: 0.7037 - val_loss: 1.1074 - val_accuracy: 0.5818\n",
      "Epoch 237/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8557 - accuracy: 0.7016\n",
      "Yes You are here 0.701646089553833 0.581818163394928\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 0.8557 - accuracy: 0.7016 - val_loss: 1.1235 - val_accuracy: 0.5818\n",
      "Epoch 238/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.7078\n",
      "Yes You are here 0.7078189253807068 0.581818163394928\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.8715 - accuracy: 0.7078 - val_loss: 1.1466 - val_accuracy: 0.5818\n",
      "Epoch 239/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8474 - accuracy: 0.7078\n",
      "Yes You are here 0.7078189253807068 0.581818163394928\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.8474 - accuracy: 0.7078 - val_loss: 1.1086 - val_accuracy: 0.5818\n",
      "Epoch 240/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8420 - accuracy: 0.7119\n",
      "Yes You are here 0.7119341492652893 0.581818163394928\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.8420 - accuracy: 0.7119 - val_loss: 1.0941 - val_accuracy: 0.5818\n",
      "Epoch 241/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8368 - accuracy: 0.7058\n",
      "Yes You are here 0.7057613134384155 0.6000000238418579\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.8368 - accuracy: 0.7058 - val_loss: 1.0852 - val_accuracy: 0.6000\n",
      "Epoch 242/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8329 - accuracy: 0.7119\n",
      "Yes You are here 0.7119341492652893 0.6000000238418579\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.8329 - accuracy: 0.7119 - val_loss: 1.1145 - val_accuracy: 0.6000\n",
      "Epoch 243/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8365 - accuracy: 0.7078\n",
      "Yes You are here 0.7078189253807068 0.6000000238418579\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.8365 - accuracy: 0.7078 - val_loss: 1.0790 - val_accuracy: 0.6000\n",
      "Epoch 244/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8390 - accuracy: 0.7016\n",
      "Yes You are here 0.701646089553833 0.6181818246841431\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.8390 - accuracy: 0.7016 - val_loss: 1.1181 - val_accuracy: 0.6182\n",
      "Epoch 245/15000\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.8215 - accuracy: 0.7160 - val_loss: 1.0883 - val_accuracy: 0.6000\n",
      "Epoch 246/15000\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.8142 - accuracy: 0.7181 - val_loss: 1.0867 - val_accuracy: 0.6000\n",
      "Epoch 247/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.8129 - accuracy: 0.7099 - val_loss: 1.0592 - val_accuracy: 0.6000\n",
      "Epoch 248/15000\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.8053 - accuracy: 0.7119 - val_loss: 1.0606 - val_accuracy: 0.6000\n",
      "Epoch 249/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 0.7989 - accuracy: 0.7099 - val_loss: 1.0686 - val_accuracy: 0.6000\n",
      "Epoch 250/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7969 - accuracy: 0.7078\n",
      "Yes You are here 0.7078189253807068 0.6181818246841431\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.7969 - accuracy: 0.7078 - val_loss: 1.0300 - val_accuracy: 0.6182\n",
      "Epoch 251/15000\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.7922 - accuracy: 0.7119 - val_loss: 1.0946 - val_accuracy: 0.6000\n",
      "Epoch 252/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7878 - accuracy: 0.7243\n",
      "Yes You are here 0.7242798209190369 0.6181818246841431\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.7878 - accuracy: 0.7243 - val_loss: 1.0200 - val_accuracy: 0.6182\n",
      "Epoch 253/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.7783 - accuracy: 0.7263 - val_loss: 1.0561 - val_accuracy: 0.5818\n",
      "Epoch 254/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1082 - accuracy: 0.7119\n",
      "Yes You are here 0.7119341492652893 0.6181818246841431\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 1.1082 - accuracy: 0.7119 - val_loss: 1.0338 - val_accuracy: 0.6182\n",
      "Epoch 255/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2993 - accuracy: 0.6687\n",
      "Yes You are here 0.6687242984771729 0.6181818246841431\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 1.2993 - accuracy: 0.6687 - val_loss: 1.1189 - val_accuracy: 0.6182\n",
      "Epoch 256/15000\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 1.2306 - accuracy: 0.6975 - val_loss: 1.0961 - val_accuracy: 0.6000\n",
      "Epoch 257/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.9973 - accuracy: 0.6770 - val_loss: 1.2457 - val_accuracy: 0.5818\n",
      "Epoch 258/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.9495 - accuracy: 0.6728 - val_loss: 1.0782 - val_accuracy: 0.6000\n",
      "Epoch 259/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.9012 - accuracy: 0.6893 - val_loss: 1.4769 - val_accuracy: 0.6000\n",
      "Epoch 260/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.8889 - accuracy: 0.6996 - val_loss: 1.0375 - val_accuracy: 0.6000\n",
      "Epoch 261/15000\n",
      "16/16 [==============================] - 3s 202ms/step - loss: 0.8463 - accuracy: 0.6934 - val_loss: 1.2735 - val_accuracy: 0.6000\n",
      "Epoch 262/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.6914\n",
      "Yes You are here 0.6913580298423767 0.6181818246841431\n",
      "16/16 [==============================] - 3s 195ms/step - loss: 0.8623 - accuracy: 0.6914 - val_loss: 1.0513 - val_accuracy: 0.6182\n",
      "Epoch 263/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8497 - accuracy: 0.6975\n",
      "Yes You are here 0.6975308656692505 0.6181818246841431\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.8497 - accuracy: 0.6975 - val_loss: 1.3298 - val_accuracy: 0.6182\n",
      "Epoch 264/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.8248 - accuracy: 0.7058 - val_loss: 1.0386 - val_accuracy: 0.6000\n",
      "Epoch 265/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.8224 - accuracy: 0.6975 - val_loss: 1.2372 - val_accuracy: 0.6000\n",
      "Epoch 266/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.7901 - accuracy: 0.7099 - val_loss: 1.1214 - val_accuracy: 0.6000\n",
      "Epoch 267/15000\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.7795 - accuracy: 0.7099 - val_loss: 1.1859 - val_accuracy: 0.6000\n",
      "Epoch 268/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.8764 - accuracy: 0.7037 - val_loss: 1.1070 - val_accuracy: 0.6000\n",
      "Epoch 269/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.9394 - accuracy: 0.7016 - val_loss: 1.0542 - val_accuracy: 0.6000\n",
      "Epoch 270/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7872 - accuracy: 0.7160\n",
      "Yes You are here 0.7160493731498718 0.6181818246841431\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.7872 - accuracy: 0.7160 - val_loss: 1.2742 - val_accuracy: 0.6182\n",
      "Epoch 271/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7764 - accuracy: 0.7181\n",
      "Yes You are here 0.7181069850921631 0.6363636255264282\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.7764 - accuracy: 0.7181 - val_loss: 1.0483 - val_accuracy: 0.6364\n",
      "Epoch 272/15000\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.7566 - accuracy: 0.7263 - val_loss: 1.1308 - val_accuracy: 0.6182\n",
      "Epoch 273/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7503 - accuracy: 0.7181\n",
      "Yes You are here 0.7181069850921631 0.6545454263687134\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.7503 - accuracy: 0.7181 - val_loss: 1.1248 - val_accuracy: 0.6545\n",
      "Epoch 274/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7454 - accuracy: 0.7263\n",
      "Yes You are here 0.7263374328613281 0.6545454263687134\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.7454 - accuracy: 0.7263 - val_loss: 1.1134 - val_accuracy: 0.6545\n",
      "Epoch 275/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.7284\n",
      "Yes You are here 0.7283950448036194 0.6545454263687134\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.7374 - accuracy: 0.7284 - val_loss: 1.1045 - val_accuracy: 0.6545\n",
      "Epoch 276/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7344 - accuracy: 0.7325\n",
      "Yes You are here 0.7325102686882019 0.6545454263687134\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.7344 - accuracy: 0.7325 - val_loss: 1.0785 - val_accuracy: 0.6545\n",
      "Epoch 277/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7321 - accuracy: 0.7284\n",
      "Yes You are here 0.7283950448036194 0.6545454263687134\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.7321 - accuracy: 0.7284 - val_loss: 1.0841 - val_accuracy: 0.6545\n",
      "Epoch 278/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.7305\n",
      "Yes You are here 0.7304526567459106 0.6545454263687134\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.7249 - accuracy: 0.7305 - val_loss: 1.0546 - val_accuracy: 0.6545\n",
      "Epoch 279/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7194 - accuracy: 0.7346\n",
      "Yes You are here 0.7345678806304932 0.6545454263687134\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.7194 - accuracy: 0.7346 - val_loss: 1.0643 - val_accuracy: 0.6545\n",
      "Epoch 280/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7175 - accuracy: 0.7284\n",
      "Yes You are here 0.7283950448036194 0.6545454263687134\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.7175 - accuracy: 0.7284 - val_loss: 1.0224 - val_accuracy: 0.6545\n",
      "Epoch 281/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.7284\n",
      "Yes You are here 0.7283950448036194 0.6545454263687134\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.7148 - accuracy: 0.7284 - val_loss: 1.0148 - val_accuracy: 0.6545\n",
      "Epoch 282/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7069 - accuracy: 0.7325\n",
      "Yes You are here 0.7325102686882019 0.6545454263687134\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 0.7069 - accuracy: 0.7325 - val_loss: 1.0207 - val_accuracy: 0.6545\n",
      "Epoch 283/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.7449\n",
      "Yes You are here 0.7448559403419495 0.6545454263687134\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 0.7028 - accuracy: 0.7449 - val_loss: 1.0143 - val_accuracy: 0.6545\n",
      "Epoch 284/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.6992 - accuracy: 0.7428 - val_loss: 0.9881 - val_accuracy: 0.6364\n",
      "Epoch 285/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.6953 - accuracy: 0.7387 - val_loss: 0.9757 - val_accuracy: 0.6364\n",
      "Epoch 286/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6922 - accuracy: 0.7469\n",
      "Yes You are here 0.7469135522842407 0.6545454263687134\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.6922 - accuracy: 0.7469 - val_loss: 0.9781 - val_accuracy: 0.6545\n",
      "Epoch 287/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6873 - accuracy: 0.7449\n",
      "Yes You are here 0.7448559403419495 0.6545454263687134\n",
      "16/16 [==============================] - 2s 158ms/step - loss: 0.6873 - accuracy: 0.7449 - val_loss: 0.9712 - val_accuracy: 0.6545\n",
      "Epoch 288/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.7469\n",
      "Yes You are here 0.7469135522842407 0.6545454263687134\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.6833 - accuracy: 0.7469 - val_loss: 0.9338 - val_accuracy: 0.6545\n",
      "Epoch 289/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6808 - accuracy: 0.7449\n",
      "Yes You are here 0.7448559403419495 0.6545454263687134\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.6808 - accuracy: 0.7449 - val_loss: 0.9365 - val_accuracy: 0.6545\n",
      "Epoch 290/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.6768 - accuracy: 0.7449 - val_loss: 0.9302 - val_accuracy: 0.6364\n",
      "Epoch 291/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.6734 - accuracy: 0.7449 - val_loss: 0.9080 - val_accuracy: 0.6364\n",
      "Epoch 292/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6701 - accuracy: 0.7449\n",
      "Yes You are here 0.7448559403419495 0.6545454263687134\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.6701 - accuracy: 0.7449 - val_loss: 0.9066 - val_accuracy: 0.6545\n",
      "Epoch 293/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.7469\n",
      "Yes You are here 0.7469135522842407 0.6727272868156433\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.6661 - accuracy: 0.7469 - val_loss: 0.8915 - val_accuracy: 0.6727\n",
      "Epoch 294/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.6642 - accuracy: 0.7469 - val_loss: 0.9022 - val_accuracy: 0.6182\n",
      "Epoch 295/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 0.6626 - accuracy: 0.7531 - val_loss: 0.9194 - val_accuracy: 0.6182\n",
      "Epoch 296/15000\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 0.6597 - accuracy: 0.7490 - val_loss: 0.8956 - val_accuracy: 0.6364\n",
      "Epoch 297/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.6557 - accuracy: 0.7490 - val_loss: 0.8846 - val_accuracy: 0.6364\n",
      "Epoch 298/15000\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.6518 - accuracy: 0.7490 - val_loss: 0.8692 - val_accuracy: 0.6545\n",
      "Epoch 299/15000\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.6476 - accuracy: 0.7469 - val_loss: 0.8771 - val_accuracy: 0.6364\n",
      "Epoch 300/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.6450 - accuracy: 0.7490 - val_loss: 0.8580 - val_accuracy: 0.6545\n",
      "Epoch 301/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.6407 - accuracy: 0.7469 - val_loss: 0.8601 - val_accuracy: 0.6545\n",
      "Epoch 302/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.6375 - accuracy: 0.7469 - val_loss: 0.8473 - val_accuracy: 0.6545\n",
      "Epoch 303/15000\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 0.6367 - accuracy: 0.7510 - val_loss: 0.8680 - val_accuracy: 0.6364\n",
      "Epoch 304/15000\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.6399 - accuracy: 0.7510 - val_loss: 0.8975 - val_accuracy: 0.6364\n",
      "Epoch 305/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.6384 - accuracy: 0.7490 - val_loss: 0.8783 - val_accuracy: 0.6545\n",
      "Epoch 306/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.7531\n",
      "Yes You are here 0.7530864477157593 0.6727272868156433\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.6319 - accuracy: 0.7531 - val_loss: 0.8434 - val_accuracy: 0.6727\n",
      "Epoch 307/15000\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 0.6263 - accuracy: 0.7572 - val_loss: 0.8321 - val_accuracy: 0.6545\n",
      "Epoch 308/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6225 - accuracy: 0.7572\n",
      "Yes You are here 0.7572016716003418 0.6727272868156433\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.6225 - accuracy: 0.7572 - val_loss: 0.8274 - val_accuracy: 0.6727\n",
      "Epoch 309/15000\n",
      "16/16 [==============================] - 3s 205ms/step - loss: 0.6191 - accuracy: 0.7593 - val_loss: 0.8211 - val_accuracy: 0.6545\n",
      "Epoch 310/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6172 - accuracy: 0.7531\n",
      "Yes You are here 0.7530864477157593 0.6727272868156433\n",
      "16/16 [==============================] - 3s 196ms/step - loss: 0.6172 - accuracy: 0.7531 - val_loss: 0.8181 - val_accuracy: 0.6727\n",
      "Epoch 311/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6132 - accuracy: 0.7551\n",
      "Yes You are here 0.7551440596580505 0.6909090876579285\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.6132 - accuracy: 0.7551 - val_loss: 0.8215 - val_accuracy: 0.6909\n",
      "Epoch 312/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6092 - accuracy: 0.7551\n",
      "Yes You are here 0.7551440596580505 0.6909090876579285\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.6092 - accuracy: 0.7551 - val_loss: 0.8138 - val_accuracy: 0.6909\n",
      "Epoch 313/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.7551\n",
      "Yes You are here 0.7551440596580505 0.6909090876579285\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.6061 - accuracy: 0.7551 - val_loss: 0.8158 - val_accuracy: 0.6909\n",
      "Epoch 314/15000\n",
      "16/16 [==============================] - 3s 196ms/step - loss: 0.6027 - accuracy: 0.7510 - val_loss: 0.8080 - val_accuracy: 0.6727\n",
      "Epoch 315/15000\n",
      "16/16 [==============================] - 3s 209ms/step - loss: 0.6002 - accuracy: 0.7593 - val_loss: 0.8019 - val_accuracy: 0.6727\n",
      "Epoch 316/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5964 - accuracy: 0.7551 - val_loss: 0.7978 - val_accuracy: 0.6727\n",
      "Epoch 317/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.7593\n",
      "Yes You are here 0.7592592835426331 0.6909090876579285\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5939 - accuracy: 0.7593 - val_loss: 0.7908 - val_accuracy: 0.6909\n",
      "Epoch 318/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.7695\n",
      "Yes You are here 0.7695473432540894 0.7090908885002136\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5909 - accuracy: 0.7695 - val_loss: 0.7872 - val_accuracy: 0.7091\n",
      "Epoch 319/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.7654\n",
      "Yes You are here 0.7654321193695068 0.7090908885002136\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.5894 - accuracy: 0.7654 - val_loss: 0.7824 - val_accuracy: 0.7091\n",
      "Epoch 320/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5864 - accuracy: 0.7737\n",
      "Yes You are here 0.7736625671386719 0.7090908885002136\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.5864 - accuracy: 0.7737 - val_loss: 0.7851 - val_accuracy: 0.7091\n",
      "Epoch 321/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5813 - accuracy: 0.7757\n",
      "Yes You are here 0.7757201790809631 0.7090908885002136\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5813 - accuracy: 0.7757 - val_loss: 0.7886 - val_accuracy: 0.7091\n",
      "Epoch 322/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5790 - accuracy: 0.7778\n",
      "Yes You are here 0.7777777910232544 0.7090908885002136\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5790 - accuracy: 0.7778 - val_loss: 0.7775 - val_accuracy: 0.7091\n",
      "Epoch 323/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.7757\n",
      "Yes You are here 0.7757201790809631 0.7090908885002136\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5762 - accuracy: 0.7757 - val_loss: 0.7753 - val_accuracy: 0.7091\n",
      "Epoch 324/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5721 - accuracy: 0.7757\n",
      "Yes You are here 0.7757201790809631 0.7090908885002136\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.5721 - accuracy: 0.7757 - val_loss: 0.7699 - val_accuracy: 0.7091\n",
      "Epoch 325/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5694 - accuracy: 0.7695\n",
      "Yes You are here 0.7695473432540894 0.7090908885002136\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.5694 - accuracy: 0.7695 - val_loss: 0.7671 - val_accuracy: 0.7091\n",
      "Epoch 326/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.7757\n",
      "Yes You are here 0.7757201790809631 0.7090908885002136\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.5665 - accuracy: 0.7757 - val_loss: 0.7640 - val_accuracy: 0.7091\n",
      "Epoch 327/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.7860\n",
      "Yes You are here 0.7860082387924194 0.7090908885002136\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.5636 - accuracy: 0.7860 - val_loss: 0.7628 - val_accuracy: 0.7091\n",
      "Epoch 328/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.7942\n",
      "Yes You are here 0.7942386865615845 0.7090908885002136\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.5610 - accuracy: 0.7942 - val_loss: 0.7667 - val_accuracy: 0.7091\n",
      "Epoch 329/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.7942\n",
      "Yes You are here 0.7942386865615845 0.7272727489471436\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5588 - accuracy: 0.7942 - val_loss: 0.7679 - val_accuracy: 0.7273\n",
      "Epoch 330/15000\n",
      "16/16 [==============================] - 3s 201ms/step - loss: 0.5562 - accuracy: 0.7860 - val_loss: 0.7575 - val_accuracy: 0.7091\n",
      "Epoch 331/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.7901\n",
      "Yes You are here 0.790123462677002 0.7272727489471436\n",
      "16/16 [==============================] - 3s 199ms/step - loss: 0.5526 - accuracy: 0.7901 - val_loss: 0.7496 - val_accuracy: 0.7273\n",
      "Epoch 332/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5497 - accuracy: 0.7984 - val_loss: 0.7596 - val_accuracy: 0.7091\n",
      "Epoch 333/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5458 - accuracy: 0.8025 - val_loss: 0.7638 - val_accuracy: 0.6909\n",
      "Epoch 334/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.5443 - accuracy: 0.7984 - val_loss: 0.7886 - val_accuracy: 0.7091\n",
      "Epoch 335/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.5399 - accuracy: 0.7963 - val_loss: 0.7607 - val_accuracy: 0.7091\n",
      "Epoch 336/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.8045\n",
      "Yes You are here 0.8045267462730408 0.7454545497894287\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.5363 - accuracy: 0.8045 - val_loss: 0.7396 - val_accuracy: 0.7455\n",
      "Epoch 337/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.8025\n",
      "Yes You are here 0.8024691343307495 0.7454545497894287\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.5331 - accuracy: 0.8025 - val_loss: 0.7403 - val_accuracy: 0.7455\n",
      "Epoch 338/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5305 - accuracy: 0.8025 - val_loss: 0.7690 - val_accuracy: 0.7091\n",
      "Epoch 339/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5273 - accuracy: 0.8025 - val_loss: 0.8566 - val_accuracy: 0.7091\n",
      "Epoch 340/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5243 - accuracy: 0.8086 - val_loss: 0.7791 - val_accuracy: 0.7091\n",
      "Epoch 341/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.8128\n",
      "Yes You are here 0.8127571940422058 0.7454545497894287\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.5220 - accuracy: 0.8128 - val_loss: 0.7714 - val_accuracy: 0.7455\n",
      "Epoch 342/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.8107\n",
      "Yes You are here 0.8106995820999146 0.7636363506317139\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5189 - accuracy: 0.8107 - val_loss: 0.7557 - val_accuracy: 0.7636\n",
      "Epoch 343/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5159 - accuracy: 0.8086 - val_loss: 0.7588 - val_accuracy: 0.7455\n",
      "Epoch 344/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.8107\n",
      "Yes You are here 0.8106995820999146 0.7636363506317139\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5134 - accuracy: 0.8107 - val_loss: 0.7568 - val_accuracy: 0.7636\n",
      "Epoch 345/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5125 - accuracy: 0.8128 - val_loss: 0.8896 - val_accuracy: 0.7273\n",
      "Epoch 346/15000\n",
      "16/16 [==============================] - 3s 223ms/step - loss: 0.5085 - accuracy: 0.8086 - val_loss: 0.7924 - val_accuracy: 0.7273\n",
      "Epoch 347/15000\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.7919 - accuracy: 0.7860 - val_loss: 0.9901 - val_accuracy: 0.7091\n",
      "Epoch 348/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 2.6497 - accuracy: 0.7695 - val_loss: 1.2755 - val_accuracy: 0.6909\n",
      "Epoch 349/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 1.6625 - accuracy: 0.6790 - val_loss: 1.2075 - val_accuracy: 0.5818\n",
      "Epoch 350/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 1.2420 - accuracy: 0.6461 - val_loss: 1.1184 - val_accuracy: 0.6000\n",
      "Epoch 351/15000\n",
      "16/16 [==============================] - 3s 192ms/step - loss: 1.9314 - accuracy: 0.6955 - val_loss: 1.0730 - val_accuracy: 0.6182\n",
      "Epoch 352/15000\n",
      "16/16 [==============================] - 4s 215ms/step - loss: 9.5235 - accuracy: 0.6626 - val_loss: 3.4366 - val_accuracy: 0.6000\n",
      "Epoch 353/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 5.3317 - accuracy: 0.6502 - val_loss: 1.2054 - val_accuracy: 0.6182\n",
      "Epoch 354/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 1.1856 - accuracy: 0.6914 - val_loss: 1.1142 - val_accuracy: 0.6909\n",
      "Epoch 355/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 1.1009 - accuracy: 0.6790 - val_loss: 1.2111 - val_accuracy: 0.7091\n",
      "Epoch 356/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.9457 - accuracy: 0.7016 - val_loss: 0.9728 - val_accuracy: 0.7818\n",
      "Epoch 357/15000\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.8711 - accuracy: 0.7202 - val_loss: 0.9627 - val_accuracy: 0.7818\n",
      "Epoch 358/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 1.1121 - accuracy: 0.7202 - val_loss: 1.0159 - val_accuracy: 0.7091\n",
      "Epoch 359/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.9958 - accuracy: 0.7263 - val_loss: 0.9420 - val_accuracy: 0.7091\n",
      "Epoch 360/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8515 - accuracy: 0.7243\n",
      "Yes You are here 0.7242798209190369 0.7636363506317139\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.8515 - accuracy: 0.7243 - val_loss: 0.9402 - val_accuracy: 0.7636\n",
      "Epoch 361/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.8045 - accuracy: 0.7346 - val_loss: 1.0037 - val_accuracy: 0.7455\n",
      "Epoch 362/15000\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.7544 - accuracy: 0.7407 - val_loss: 0.9202 - val_accuracy: 0.7455\n",
      "Epoch 363/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7363 - accuracy: 0.7531\n",
      "Yes You are here 0.7530864477157593 0.7636363506317139\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.7363 - accuracy: 0.7531 - val_loss: 0.9141 - val_accuracy: 0.7636\n",
      "Epoch 364/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.7187 - accuracy: 0.7551 - val_loss: 0.9310 - val_accuracy: 0.7091\n",
      "Epoch 365/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.7016 - accuracy: 0.7654 - val_loss: 0.9064 - val_accuracy: 0.7273\n",
      "Epoch 366/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.6904 - accuracy: 0.7654 - val_loss: 0.9033 - val_accuracy: 0.7273\n",
      "Epoch 367/15000\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.6791 - accuracy: 0.7695 - val_loss: 0.9064 - val_accuracy: 0.7273\n",
      "Epoch 368/15000\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.6705 - accuracy: 0.7716 - val_loss: 0.9047 - val_accuracy: 0.7273\n",
      "Epoch 369/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.6632 - accuracy: 0.7798 - val_loss: 0.9060 - val_accuracy: 0.7273\n",
      "Epoch 370/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.6589 - accuracy: 0.7819 - val_loss: 0.9064 - val_accuracy: 0.6909\n",
      "Epoch 371/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.6503 - accuracy: 0.7860 - val_loss: 0.9044 - val_accuracy: 0.6909\n",
      "Epoch 372/15000\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.6455 - accuracy: 0.7860 - val_loss: 0.8897 - val_accuracy: 0.7091\n",
      "Epoch 373/15000\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.6484 - accuracy: 0.7757 - val_loss: 0.8897 - val_accuracy: 0.6909\n",
      "Epoch 374/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.6370 - accuracy: 0.7778 - val_loss: 0.8812 - val_accuracy: 0.6909\n",
      "Epoch 375/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.6303 - accuracy: 0.7716 - val_loss: 0.8841 - val_accuracy: 0.6909\n",
      "Epoch 376/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.6247 - accuracy: 0.7798 - val_loss: 0.8772 - val_accuracy: 0.6909\n",
      "Epoch 377/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.6201 - accuracy: 0.7757 - val_loss: 0.8826 - val_accuracy: 0.6727\n",
      "Epoch 378/15000\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.6169 - accuracy: 0.7840 - val_loss: 0.8713 - val_accuracy: 0.6909\n",
      "Epoch 379/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.8017 - accuracy: 0.7654 - val_loss: 1.0129 - val_accuracy: 0.7091\n",
      "Epoch 380/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 1.7651 - accuracy: 0.7305 - val_loss: 0.9865 - val_accuracy: 0.7455\n",
      "Epoch 381/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.9117 - accuracy: 0.7510 - val_loss: 0.9046 - val_accuracy: 0.7455\n",
      "Epoch 382/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.7260 - accuracy: 0.7695 - val_loss: 0.9395 - val_accuracy: 0.6909\n",
      "Epoch 383/15000\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 0.6522 - accuracy: 0.7840 - val_loss: 0.8710 - val_accuracy: 0.7091\n",
      "Epoch 384/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.6201 - accuracy: 0.7819 - val_loss: 0.8600 - val_accuracy: 0.7273\n",
      "Epoch 385/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.6113 - accuracy: 0.7922 - val_loss: 0.8549 - val_accuracy: 0.7273\n",
      "Epoch 386/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.6041 - accuracy: 0.8004 - val_loss: 0.8510 - val_accuracy: 0.7273\n",
      "Epoch 387/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.6010 - accuracy: 0.7984 - val_loss: 0.8494 - val_accuracy: 0.7273\n",
      "Epoch 388/15000\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.5983 - accuracy: 0.8004 - val_loss: 0.8460 - val_accuracy: 0.7273\n",
      "Epoch 389/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.5924 - accuracy: 0.8045 - val_loss: 0.8445 - val_accuracy: 0.7273\n",
      "Epoch 390/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5906 - accuracy: 0.7963 - val_loss: 0.8411 - val_accuracy: 0.7273\n",
      "Epoch 391/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.5872 - accuracy: 0.8025 - val_loss: 0.8395 - val_accuracy: 0.7273\n",
      "Epoch 392/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.5848 - accuracy: 0.8025 - val_loss: 0.8380 - val_accuracy: 0.7273\n",
      "Epoch 393/15000\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.5830 - accuracy: 0.8004 - val_loss: 0.8372 - val_accuracy: 0.7273\n",
      "Epoch 394/15000\n",
      "16/16 [==============================] - 4s 213ms/step - loss: 0.5798 - accuracy: 0.8045 - val_loss: 0.8364 - val_accuracy: 0.7091\n",
      "Epoch 395/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.5769 - accuracy: 0.8066 - val_loss: 0.8356 - val_accuracy: 0.7273\n",
      "Epoch 396/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.5754 - accuracy: 0.8148 - val_loss: 0.8343 - val_accuracy: 0.7091\n",
      "Epoch 397/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5726 - accuracy: 0.8128 - val_loss: 0.8336 - val_accuracy: 0.7091\n",
      "Epoch 398/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.5702 - accuracy: 0.8107 - val_loss: 0.8315 - val_accuracy: 0.7091\n",
      "Epoch 399/15000\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.5681 - accuracy: 0.8169 - val_loss: 0.8308 - val_accuracy: 0.7091\n",
      "Epoch 400/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.5650 - accuracy: 0.8086 - val_loss: 0.8302 - val_accuracy: 0.7091\n",
      "Epoch 401/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5630 - accuracy: 0.8148 - val_loss: 0.8293 - val_accuracy: 0.7091\n",
      "Epoch 402/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5608 - accuracy: 0.8169 - val_loss: 0.8272 - val_accuracy: 0.7273\n",
      "Epoch 403/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5589 - accuracy: 0.8210 - val_loss: 0.8257 - val_accuracy: 0.7091\n",
      "Epoch 404/15000\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.5567 - accuracy: 0.8128 - val_loss: 0.8251 - val_accuracy: 0.7091\n",
      "Epoch 405/15000\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.5542 - accuracy: 0.8169 - val_loss: 0.8237 - val_accuracy: 0.7091\n",
      "Epoch 406/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5528 - accuracy: 0.8148 - val_loss: 0.8210 - val_accuracy: 0.7091\n",
      "Epoch 407/15000\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.5511 - accuracy: 0.8148 - val_loss: 0.8209 - val_accuracy: 0.7273\n",
      "Epoch 408/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.5488 - accuracy: 0.8148 - val_loss: 0.8184 - val_accuracy: 0.7091\n",
      "Epoch 409/15000\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 0.5460 - accuracy: 0.8189 - val_loss: 0.8176 - val_accuracy: 0.7273\n",
      "Epoch 410/15000\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.5435 - accuracy: 0.8210 - val_loss: 0.8171 - val_accuracy: 0.7455\n",
      "Epoch 411/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.5425 - accuracy: 0.8251 - val_loss: 0.8187 - val_accuracy: 0.7273\n",
      "Epoch 412/15000\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.5401 - accuracy: 0.8313 - val_loss: 0.8182 - val_accuracy: 0.7273\n",
      "Epoch 413/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.5392 - accuracy: 0.8272 - val_loss: 0.8165 - val_accuracy: 0.7273\n",
      "Epoch 414/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5363 - accuracy: 0.8292 - val_loss: 0.8148 - val_accuracy: 0.7455\n",
      "Epoch 415/15000\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.5328 - accuracy: 0.8189 - val_loss: 0.8133 - val_accuracy: 0.7455\n",
      "Epoch 416/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5311 - accuracy: 0.8292 - val_loss: 0.8128 - val_accuracy: 0.7455\n",
      "Epoch 417/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.8333\n",
      "Yes You are here 0.8333333134651184 0.7636363506317139\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5291 - accuracy: 0.8333 - val_loss: 0.8119 - val_accuracy: 0.7636\n",
      "Epoch 418/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.8333\n",
      "Yes You are here 0.8333333134651184 0.7636363506317139\n",
      "16/16 [==============================] - 2s 157ms/step - loss: 0.5271 - accuracy: 0.8333 - val_loss: 0.8116 - val_accuracy: 0.7636\n",
      "Epoch 419/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.8292\n",
      "Yes You are here 0.8292180895805359 0.7636363506317139\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5257 - accuracy: 0.8292 - val_loss: 0.8107 - val_accuracy: 0.7636\n",
      "Epoch 420/15000\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 0.5230 - accuracy: 0.8292 - val_loss: 0.8063 - val_accuracy: 0.7455\n",
      "Epoch 421/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5215 - accuracy: 0.8395 - val_loss: 0.8072 - val_accuracy: 0.7455\n",
      "Epoch 422/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.8354\n",
      "Yes You are here 0.8353909254074097 0.7818182110786438\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5194 - accuracy: 0.8354 - val_loss: 0.8085 - val_accuracy: 0.7818\n",
      "Epoch 423/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.8374\n",
      "Yes You are here 0.8374485373497009 0.7818182110786438\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5165 - accuracy: 0.8374 - val_loss: 0.8093 - val_accuracy: 0.7818\n",
      "Epoch 424/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.8374\n",
      "Yes You are here 0.8374485373497009 0.7818182110786438\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.5146 - accuracy: 0.8374 - val_loss: 0.8088 - val_accuracy: 0.7818\n",
      "Epoch 425/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.8395\n",
      "Yes You are here 0.8395061492919922 0.7818182110786438\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.5123 - accuracy: 0.8395 - val_loss: 0.8094 - val_accuracy: 0.7818\n",
      "Epoch 426/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.8374\n",
      "Yes You are here 0.8374485373497009 0.7818182110786438\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.5105 - accuracy: 0.8374 - val_loss: 0.8085 - val_accuracy: 0.7818\n",
      "Epoch 427/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5085 - accuracy: 0.8374\n",
      "Yes You are here 0.8374485373497009 0.7818182110786438\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5085 - accuracy: 0.8374 - val_loss: 0.8071 - val_accuracy: 0.7818\n",
      "Epoch 428/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.5063 - accuracy: 0.8374 - val_loss: 0.8066 - val_accuracy: 0.7636\n",
      "Epoch 429/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.5051 - accuracy: 0.8292 - val_loss: 0.8067 - val_accuracy: 0.7636\n",
      "Epoch 430/15000\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.5027 - accuracy: 0.8354 - val_loss: 0.8066 - val_accuracy: 0.7636\n",
      "Epoch 431/15000\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.5013 - accuracy: 0.8374 - val_loss: 0.8073 - val_accuracy: 0.7636\n",
      "Epoch 432/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.4989 - accuracy: 0.8436 - val_loss: 0.8068 - val_accuracy: 0.7636\n",
      "Epoch 433/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.4972 - accuracy: 0.8436 - val_loss: 0.8073 - val_accuracy: 0.7636\n",
      "Epoch 434/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4966 - accuracy: 0.8498 - val_loss: 0.8088 - val_accuracy: 0.7636\n",
      "Epoch 435/15000\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.4943 - accuracy: 0.8519 - val_loss: 0.8105 - val_accuracy: 0.7636\n",
      "Epoch 436/15000\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.4908 - accuracy: 0.8457 - val_loss: 0.8081 - val_accuracy: 0.7636\n",
      "Epoch 437/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4896 - accuracy: 0.8457 - val_loss: 0.8081 - val_accuracy: 0.7636\n",
      "Epoch 438/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.8560\n",
      "Yes You are here 0.855967104434967 0.7818182110786438\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.4876 - accuracy: 0.8560 - val_loss: 0.8075 - val_accuracy: 0.7818\n",
      "Epoch 439/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.8560\n",
      "Yes You are here 0.855967104434967 0.7818182110786438\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.4854 - accuracy: 0.8560 - val_loss: 0.8060 - val_accuracy: 0.7818\n",
      "Epoch 440/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4840 - accuracy: 0.8539\n",
      "Yes You are here 0.8539094924926758 0.7818182110786438\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.4840 - accuracy: 0.8539 - val_loss: 0.8058 - val_accuracy: 0.7818\n",
      "Epoch 441/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4822 - accuracy: 0.8560\n",
      "Yes You are here 0.855967104434967 0.7818182110786438\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.4822 - accuracy: 0.8560 - val_loss: 0.8060 - val_accuracy: 0.7818\n",
      "Epoch 442/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.8539\n",
      "Yes You are here 0.8539094924926758 0.7818182110786438\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.4802 - accuracy: 0.8539 - val_loss: 0.8069 - val_accuracy: 0.7818\n",
      "Epoch 443/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.8601\n",
      "Yes You are here 0.8600823283195496 0.7818182110786438\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4783 - accuracy: 0.8601 - val_loss: 0.8073 - val_accuracy: 0.7818\n",
      "Epoch 444/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4767 - accuracy: 0.8580\n",
      "Yes You are here 0.8580247163772583 0.7818182110786438\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.4767 - accuracy: 0.8580 - val_loss: 0.8062 - val_accuracy: 0.7818\n",
      "Epoch 445/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.8580\n",
      "Yes You are here 0.8580247163772583 0.7818182110786438\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.4751 - accuracy: 0.8580 - val_loss: 0.8052 - val_accuracy: 0.7818\n",
      "Epoch 446/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4732 - accuracy: 0.8539\n",
      "Yes You are here 0.8539094924926758 0.7818182110786438\n",
      "16/16 [==============================] - 4s 279ms/step - loss: 0.4732 - accuracy: 0.8539 - val_loss: 0.8066 - val_accuracy: 0.7818\n",
      "Epoch 447/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4717 - accuracy: 0.8498\n",
      "Yes You are here 0.8497942090034485 0.7818182110786438\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.4717 - accuracy: 0.8498 - val_loss: 0.8085 - val_accuracy: 0.7818\n",
      "Epoch 448/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.8601\n",
      "Yes You are here 0.8600823283195496 0.7818182110786438\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4702 - accuracy: 0.8601 - val_loss: 0.8067 - val_accuracy: 0.7818\n",
      "Epoch 449/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4681 - accuracy: 0.8580\n",
      "Yes You are here 0.8580247163772583 0.7818182110786438\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4681 - accuracy: 0.8580 - val_loss: 0.8088 - val_accuracy: 0.7818\n",
      "Epoch 450/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4661 - accuracy: 0.8642\n",
      "Yes You are here 0.8641975522041321 0.7818182110786438\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4661 - accuracy: 0.8642 - val_loss: 0.8099 - val_accuracy: 0.7818\n",
      "Epoch 451/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4648 - accuracy: 0.8560\n",
      "Yes You are here 0.855967104434967 0.7818182110786438\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.4648 - accuracy: 0.8560 - val_loss: 0.8095 - val_accuracy: 0.7818\n",
      "Epoch 452/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.8621\n",
      "Yes You are here 0.8621399402618408 0.7818182110786438\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.4631 - accuracy: 0.8621 - val_loss: 0.8094 - val_accuracy: 0.7818\n",
      "Epoch 453/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.8621\n",
      "Yes You are here 0.8621399402618408 0.7818182110786438\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.4616 - accuracy: 0.8621 - val_loss: 0.8127 - val_accuracy: 0.7818\n",
      "Epoch 454/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.8663\n",
      "Yes You are here 0.8662551641464233 0.7818182110786438\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4594 - accuracy: 0.8663 - val_loss: 0.8126 - val_accuracy: 0.7818\n",
      "Epoch 455/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.8642\n",
      "Yes You are here 0.8641975522041321 0.7818182110786438\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4583 - accuracy: 0.8642 - val_loss: 0.8131 - val_accuracy: 0.7818\n",
      "Epoch 456/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4560 - accuracy: 0.8601\n",
      "Yes You are here 0.8600823283195496 0.7818182110786438\n",
      "16/16 [==============================] - 3s 197ms/step - loss: 0.4560 - accuracy: 0.8601 - val_loss: 0.8108 - val_accuracy: 0.7818\n",
      "Epoch 457/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.8621\n",
      "Yes You are here 0.8621399402618408 0.7818182110786438\n",
      "16/16 [==============================] - 3s 210ms/step - loss: 0.4537 - accuracy: 0.8621 - val_loss: 0.8107 - val_accuracy: 0.7818\n",
      "Epoch 458/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4525 - accuracy: 0.8580\n",
      "Yes You are here 0.8580247163772583 0.7818182110786438\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4525 - accuracy: 0.8580 - val_loss: 0.8126 - val_accuracy: 0.7818\n",
      "Epoch 459/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.8539\n",
      "Yes You are here 0.8539094924926758 0.7818182110786438\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4508 - accuracy: 0.8539 - val_loss: 0.8131 - val_accuracy: 0.7818\n",
      "Epoch 460/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.8724\n",
      "Yes You are here 0.8724279999732971 0.7818182110786438\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4491 - accuracy: 0.8724 - val_loss: 0.8136 - val_accuracy: 0.7818\n",
      "Epoch 461/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8745\n",
      "Yes You are here 0.8744856119155884 0.7818182110786438\n",
      "16/16 [==============================] - 3s 193ms/step - loss: 0.4469 - accuracy: 0.8745 - val_loss: 0.8168 - val_accuracy: 0.7818\n",
      "Epoch 462/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.8683\n",
      "Yes You are here 0.8683127760887146 0.7818182110786438\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.4451 - accuracy: 0.8683 - val_loss: 0.8167 - val_accuracy: 0.7818\n",
      "Epoch 463/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4439 - accuracy: 0.8724\n",
      "Yes You are here 0.8724279999732971 0.7818182110786438\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.4439 - accuracy: 0.8724 - val_loss: 0.8191 - val_accuracy: 0.7818\n",
      "Epoch 464/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.8683\n",
      "Yes You are here 0.8683127760887146 0.7818182110786438\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4420 - accuracy: 0.8683 - val_loss: 0.8230 - val_accuracy: 0.7818\n",
      "Epoch 465/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.8560\n",
      "Yes You are here 0.855967104434967 0.7818182110786438\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4398 - accuracy: 0.8560 - val_loss: 0.8249 - val_accuracy: 0.7818\n",
      "Epoch 466/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.8560\n",
      "Yes You are here 0.855967104434967 0.7818182110786438\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.4386 - accuracy: 0.8560 - val_loss: 0.8257 - val_accuracy: 0.7818\n",
      "Epoch 467/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.8580\n",
      "Yes You are here 0.8580247163772583 0.7818182110786438\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.4367 - accuracy: 0.8580 - val_loss: 0.8270 - val_accuracy: 0.7818\n",
      "Epoch 468/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.8580\n",
      "Yes You are here 0.8580247163772583 0.7818182110786438\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4350 - accuracy: 0.8580 - val_loss: 0.8335 - val_accuracy: 0.7818\n",
      "Epoch 469/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4333 - accuracy: 0.8663\n",
      "Yes You are here 0.8662551641464233 0.7818182110786438\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4333 - accuracy: 0.8663 - val_loss: 0.8330 - val_accuracy: 0.7818\n",
      "Epoch 470/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4325 - accuracy: 0.8683\n",
      "Yes You are here 0.8683127760887146 0.7818182110786438\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4325 - accuracy: 0.8683 - val_loss: 0.8412 - val_accuracy: 0.7818\n",
      "Epoch 471/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8704\n",
      "Yes You are here 0.8703703880310059 0.800000011920929\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4316 - accuracy: 0.8704 - val_loss: 0.8369 - val_accuracy: 0.8000\n",
      "Epoch 472/15000\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.4404 - accuracy: 0.8663 - val_loss: 0.8373 - val_accuracy: 0.7818\n",
      "Epoch 473/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8704\n",
      "Yes You are here 0.8703703880310059 0.800000011920929\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4298 - accuracy: 0.8704 - val_loss: 0.8364 - val_accuracy: 0.8000\n",
      "Epoch 474/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4253 - accuracy: 0.8807\n",
      "Yes You are here 0.8806584477424622 0.800000011920929\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4253 - accuracy: 0.8807 - val_loss: 0.8426 - val_accuracy: 0.8000\n",
      "Epoch 475/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.8683\n",
      "Yes You are here 0.8683127760887146 0.800000011920929\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4235 - accuracy: 0.8683 - val_loss: 0.8442 - val_accuracy: 0.8000\n",
      "Epoch 476/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4216 - accuracy: 0.8642\n",
      "Yes You are here 0.8641975522041321 0.800000011920929\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4216 - accuracy: 0.8642 - val_loss: 0.8467 - val_accuracy: 0.8000\n",
      "Epoch 477/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8683\n",
      "Yes You are here 0.8683127760887146 0.800000011920929\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.4191 - accuracy: 0.8683 - val_loss: 0.8479 - val_accuracy: 0.8000\n",
      "Epoch 478/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4177 - accuracy: 0.8724\n",
      "Yes You are here 0.8724279999732971 0.800000011920929\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4177 - accuracy: 0.8724 - val_loss: 0.8543 - val_accuracy: 0.8000\n",
      "Epoch 479/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8745\n",
      "Yes You are here 0.8744856119155884 0.800000011920929\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.4159 - accuracy: 0.8745 - val_loss: 0.8602 - val_accuracy: 0.8000\n",
      "Epoch 480/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8765\n",
      "Yes You are here 0.8765432238578796 0.800000011920929\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.4144 - accuracy: 0.8765 - val_loss: 0.8591 - val_accuracy: 0.8000\n",
      "Epoch 481/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.8745\n",
      "Yes You are here 0.8744856119155884 0.800000011920929\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4124 - accuracy: 0.8745 - val_loss: 0.8615 - val_accuracy: 0.8000\n",
      "Epoch 482/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8807\n",
      "Yes You are here 0.8806584477424622 0.8363636136054993\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.4107 - accuracy: 0.8807 - val_loss: 0.8573 - val_accuracy: 0.8364\n",
      "Epoch 483/15000\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.4121 - accuracy: 0.8745 - val_loss: 0.9614 - val_accuracy: 0.8182\n",
      "Epoch 484/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.4289 - accuracy: 0.8704 - val_loss: 0.8616 - val_accuracy: 0.8182\n",
      "Epoch 485/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4068 - accuracy: 0.8827\n",
      "Yes You are here 0.8827160596847534 0.8363636136054993\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4068 - accuracy: 0.8827 - val_loss: 0.8669 - val_accuracy: 0.8364\n",
      "Epoch 486/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.4047 - accuracy: 0.8786 - val_loss: 0.8798 - val_accuracy: 0.8000\n",
      "Epoch 487/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4026 - accuracy: 0.8807\n",
      "Yes You are here 0.8806584477424622 0.8363636136054993\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.4026 - accuracy: 0.8807 - val_loss: 0.8816 - val_accuracy: 0.8364\n",
      "Epoch 488/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.8827\n",
      "Yes You are here 0.8827160596847534 0.8363636136054993\n",
      "16/16 [==============================] - 3s 192ms/step - loss: 0.4002 - accuracy: 0.8827 - val_loss: 0.8863 - val_accuracy: 0.8364\n",
      "Epoch 489/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.8868\n",
      "Yes You are here 0.8868312835693359 0.8545454740524292\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4005 - accuracy: 0.8868 - val_loss: 0.8933 - val_accuracy: 0.8545\n",
      "Epoch 490/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.3998 - accuracy: 0.8827 - val_loss: 0.8802 - val_accuracy: 0.8182\n",
      "Epoch 491/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.3991 - accuracy: 0.8745 - val_loss: 0.8792 - val_accuracy: 0.8364\n",
      "Epoch 492/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3957 - accuracy: 0.8889\n",
      "Yes You are here 0.8888888955116272 0.8545454740524292\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.3957 - accuracy: 0.8889 - val_loss: 0.8906 - val_accuracy: 0.8545\n",
      "Epoch 493/15000\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.3984 - accuracy: 0.8889 - val_loss: 0.8783 - val_accuracy: 0.8364\n",
      "Epoch 494/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.8939 - accuracy: 0.8333 - val_loss: 0.9848 - val_accuracy: 0.6909\n",
      "Epoch 495/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.7434 - accuracy: 0.7984 - val_loss: 0.7950 - val_accuracy: 0.7091\n",
      "Epoch 496/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5973 - accuracy: 0.8313 - val_loss: 0.6932 - val_accuracy: 0.7455\n",
      "Epoch 497/15000\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.4827 - accuracy: 0.8663 - val_loss: 0.6511 - val_accuracy: 0.7818\n",
      "Epoch 498/15000\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.4489 - accuracy: 0.8827 - val_loss: 0.6329 - val_accuracy: 0.8364\n",
      "Epoch 499/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4363 - accuracy: 0.8663 - val_loss: 0.6275 - val_accuracy: 0.8364\n",
      "Epoch 500/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8786\n",
      "Yes You are here 0.8786008358001709 0.8545454740524292\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4271 - accuracy: 0.8786 - val_loss: 0.6275 - val_accuracy: 0.8545\n",
      "Epoch 501/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8827\n",
      "Yes You are here 0.8827160596847534 0.8545454740524292\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4223 - accuracy: 0.8827 - val_loss: 0.6236 - val_accuracy: 0.8545\n",
      "Epoch 502/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.8868\n",
      "Yes You are here 0.8868312835693359 0.8545454740524292\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4160 - accuracy: 0.8868 - val_loss: 0.6257 - val_accuracy: 0.8545\n",
      "Epoch 503/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.8868\n",
      "Yes You are here 0.8868312835693359 0.8545454740524292\n",
      "16/16 [==============================] - 4s 280ms/step - loss: 0.4119 - accuracy: 0.8868 - val_loss: 0.6320 - val_accuracy: 0.8545\n",
      "Epoch 504/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4079 - accuracy: 0.8889\n",
      "Yes You are here 0.8888888955116272 0.8545454740524292\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4079 - accuracy: 0.8889 - val_loss: 0.6367 - val_accuracy: 0.8545\n",
      "Epoch 505/15000\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.4054 - accuracy: 0.8909 - val_loss: 0.6434 - val_accuracy: 0.8364\n",
      "Epoch 506/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8889\n",
      "Yes You are here 0.8888888955116272 0.8545454740524292\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4029 - accuracy: 0.8889 - val_loss: 0.6492 - val_accuracy: 0.8545\n",
      "Epoch 507/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8909\n",
      "Yes You are here 0.8909465074539185 0.8545454740524292\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4004 - accuracy: 0.8909 - val_loss: 0.6527 - val_accuracy: 0.8545\n",
      "Epoch 508/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3980 - accuracy: 0.8909\n",
      "Yes You are here 0.8909465074539185 0.8545454740524292\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.3980 - accuracy: 0.8909 - val_loss: 0.6578 - val_accuracy: 0.8545\n",
      "Epoch 509/15000\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.3959 - accuracy: 0.8909 - val_loss: 0.6626 - val_accuracy: 0.8364\n",
      "Epoch 510/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3938 - accuracy: 0.8909\n",
      "Yes You are here 0.8909465074539185 0.8545454740524292\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.3938 - accuracy: 0.8909 - val_loss: 0.6674 - val_accuracy: 0.8545\n",
      "Epoch 511/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.3914 - accuracy: 0.8868 - val_loss: 0.6716 - val_accuracy: 0.8364\n",
      "Epoch 512/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3903 - accuracy: 0.8889\n",
      "Yes You are here 0.8888888955116272 0.8727272748947144\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.3903 - accuracy: 0.8889 - val_loss: 0.6782 - val_accuracy: 0.8727\n",
      "Epoch 513/15000\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 0.3878 - accuracy: 0.8930 - val_loss: 0.6818 - val_accuracy: 0.8545\n",
      "Epoch 514/15000\n",
      "16/16 [==============================] - 3s 195ms/step - loss: 0.3854 - accuracy: 0.8951 - val_loss: 0.6917 - val_accuracy: 0.8545\n",
      "Epoch 515/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3839 - accuracy: 0.8951 - val_loss: 0.6979 - val_accuracy: 0.8545\n",
      "Epoch 516/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.3816 - accuracy: 0.8951 - val_loss: 0.7058 - val_accuracy: 0.8545\n",
      "Epoch 517/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3797 - accuracy: 0.8930\n",
      "Yes You are here 0.8930041193962097 0.8727272748947144\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3797 - accuracy: 0.8930 - val_loss: 0.7140 - val_accuracy: 0.8727\n",
      "Epoch 518/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8951\n",
      "Yes You are here 0.895061731338501 0.8727272748947144\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.3771 - accuracy: 0.8951 - val_loss: 0.7101 - val_accuracy: 0.8727\n",
      "Epoch 519/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3756 - accuracy: 0.8971\n",
      "Yes You are here 0.8971193432807922 0.8727272748947144\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.3756 - accuracy: 0.8971 - val_loss: 0.7170 - val_accuracy: 0.8727\n",
      "Epoch 520/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.8992\n",
      "Yes You are here 0.8991769552230835 0.8909090757369995\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3734 - accuracy: 0.8992 - val_loss: 0.7283 - val_accuracy: 0.8909\n",
      "Epoch 521/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.3717 - accuracy: 0.8971 - val_loss: 0.7377 - val_accuracy: 0.8727\n",
      "Epoch 522/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8971\n",
      "Yes You are here 0.8971193432807922 0.8909090757369995\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.3691 - accuracy: 0.8971 - val_loss: 0.7479 - val_accuracy: 0.8909\n",
      "Epoch 523/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3675 - accuracy: 0.8971\n",
      "Yes You are here 0.8971193432807922 0.8909090757369995\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.3675 - accuracy: 0.8971 - val_loss: 0.7558 - val_accuracy: 0.8909\n",
      "Epoch 524/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.8971\n",
      "Yes You are here 0.8971193432807922 0.8909090757369995\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.3658 - accuracy: 0.8971 - val_loss: 0.7648 - val_accuracy: 0.8909\n",
      "Epoch 525/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.3640 - accuracy: 0.8971 - val_loss: 0.7688 - val_accuracy: 0.8727\n",
      "Epoch 526/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3623 - accuracy: 0.9012\n",
      "Yes You are here 0.9012345671653748 0.8909090757369995\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.3623 - accuracy: 0.9012 - val_loss: 0.7813 - val_accuracy: 0.8909\n",
      "Epoch 527/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8971\n",
      "Yes You are here 0.8971193432807922 0.8909090757369995\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.3602 - accuracy: 0.8971 - val_loss: 0.7908 - val_accuracy: 0.8909\n",
      "Epoch 528/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3585 - accuracy: 0.8992\n",
      "Yes You are here 0.8991769552230835 0.8909090757369995\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3585 - accuracy: 0.8992 - val_loss: 0.7990 - val_accuracy: 0.8909\n",
      "Epoch 529/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8971\n",
      "Yes You are here 0.8971193432807922 0.8909090757369995\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.3560 - accuracy: 0.8971 - val_loss: 0.8035 - val_accuracy: 0.8909\n",
      "Epoch 530/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8992\n",
      "Yes You are here 0.8991769552230835 0.8909090757369995\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.3548 - accuracy: 0.8992 - val_loss: 0.8137 - val_accuracy: 0.8909\n",
      "Epoch 531/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.9012\n",
      "Yes You are here 0.9012345671653748 0.8909090757369995\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.3523 - accuracy: 0.9012 - val_loss: 0.8247 - val_accuracy: 0.8909\n",
      "Epoch 532/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3513 - accuracy: 0.9012\n",
      "Yes You are here 0.9012345671653748 0.8909090757369995\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3513 - accuracy: 0.9012 - val_loss: 0.8186 - val_accuracy: 0.8909\n",
      "Epoch 533/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.8992\n",
      "Yes You are here 0.8991769552230835 0.8909090757369995\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.3493 - accuracy: 0.8992 - val_loss: 0.8278 - val_accuracy: 0.8909\n",
      "Epoch 534/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3470 - accuracy: 0.9053\n",
      "Yes You are here 0.9053497910499573 0.8909090757369995\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.3470 - accuracy: 0.9053 - val_loss: 0.8382 - val_accuracy: 0.8909\n",
      "Epoch 535/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.9053\n",
      "Yes You are here 0.9053497910499573 0.8909090757369995\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.3450 - accuracy: 0.9053 - val_loss: 0.8533 - val_accuracy: 0.8909\n",
      "Epoch 536/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.9053\n",
      "Yes You are here 0.9053497910499573 0.8909090757369995\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.3431 - accuracy: 0.9053 - val_loss: 0.8601 - val_accuracy: 0.8909\n",
      "Epoch 537/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.9012\n",
      "Yes You are here 0.9012345671653748 0.8909090757369995\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.3419 - accuracy: 0.9012 - val_loss: 0.8650 - val_accuracy: 0.8909\n",
      "Epoch 538/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 1.0061 - accuracy: 0.8272 - val_loss: 1.4678 - val_accuracy: 0.5636\n",
      "Epoch 539/15000\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 2.1107 - accuracy: 0.6276 - val_loss: 1.1668 - val_accuracy: 0.5455\n",
      "Epoch 540/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 1.2985 - accuracy: 0.6605 - val_loss: 1.1234 - val_accuracy: 0.5818\n",
      "Epoch 541/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 1.4394 - accuracy: 0.6955 - val_loss: 6.3838 - val_accuracy: 0.5273\n",
      "Epoch 542/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 15.6618 - accuracy: 0.5309 - val_loss: 5.0975 - val_accuracy: 0.4909\n",
      "Epoch 543/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 5.5146 - accuracy: 0.5679 - val_loss: 2.9883 - val_accuracy: 0.4909\n",
      "Epoch 544/15000\n",
      "16/16 [==============================] - 3s 199ms/step - loss: 3.5901 - accuracy: 0.6173 - val_loss: 2.3536 - val_accuracy: 0.5455\n",
      "Epoch 545/15000\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 1.8167 - accuracy: 0.6420 - val_loss: 1.4387 - val_accuracy: 0.6000\n",
      "Epoch 546/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 1.3612 - accuracy: 0.6728 - val_loss: 1.2402 - val_accuracy: 0.6909\n",
      "Epoch 547/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 1.4369 - accuracy: 0.6831 - val_loss: 1.0346 - val_accuracy: 0.6909\n",
      "Epoch 548/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 1.0826 - accuracy: 0.6914 - val_loss: 1.1832 - val_accuracy: 0.6909\n",
      "Epoch 549/15000\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 1.2511 - accuracy: 0.6811 - val_loss: 1.0638 - val_accuracy: 0.6909\n",
      "Epoch 550/15000\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 1.2660 - accuracy: 0.6811 - val_loss: 1.0121 - val_accuracy: 0.6727\n",
      "Epoch 551/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.9687 - accuracy: 0.6852 - val_loss: 1.0059 - val_accuracy: 0.6545\n",
      "Epoch 552/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.8863 - accuracy: 0.7058 - val_loss: 0.9771 - val_accuracy: 0.6727\n",
      "Epoch 553/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.8550 - accuracy: 0.7160 - val_loss: 0.9658 - val_accuracy: 0.7091\n",
      "Epoch 554/15000\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.8352 - accuracy: 0.7222 - val_loss: 0.9606 - val_accuracy: 0.7091\n",
      "Epoch 555/15000\n",
      "16/16 [==============================] - 3s 205ms/step - loss: 0.8276 - accuracy: 0.7222 - val_loss: 0.9528 - val_accuracy: 0.6909\n",
      "Epoch 556/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.8069 - accuracy: 0.7387 - val_loss: 0.9734 - val_accuracy: 0.6909\n",
      "Epoch 557/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.7943 - accuracy: 0.7346 - val_loss: 0.9387 - val_accuracy: 0.6909\n",
      "Epoch 558/15000\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.7755 - accuracy: 0.7449 - val_loss: 0.9326 - val_accuracy: 0.7455\n",
      "Epoch 559/15000\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.7684 - accuracy: 0.7490 - val_loss: 0.9291 - val_accuracy: 0.7455\n",
      "Epoch 560/15000\n",
      "16/16 [==============================] - 4s 219ms/step - loss: 0.7553 - accuracy: 0.7449 - val_loss: 0.9226 - val_accuracy: 0.7091\n",
      "Epoch 561/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.7512 - accuracy: 0.7407 - val_loss: 0.9173 - val_accuracy: 0.6909\n",
      "Epoch 562/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.7431 - accuracy: 0.7407 - val_loss: 0.9210 - val_accuracy: 0.7091\n",
      "Epoch 563/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.7306 - accuracy: 0.7572 - val_loss: 0.9042 - val_accuracy: 0.7273\n",
      "Epoch 564/15000\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.7240 - accuracy: 0.7613 - val_loss: 0.8993 - val_accuracy: 0.7273\n",
      "Epoch 565/15000\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.7165 - accuracy: 0.7634 - val_loss: 0.8976 - val_accuracy: 0.7273\n",
      "Epoch 566/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.7115 - accuracy: 0.7675 - val_loss: 0.8924 - val_accuracy: 0.7273\n",
      "Epoch 567/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.7059 - accuracy: 0.7695 - val_loss: 0.8884 - val_accuracy: 0.7273\n",
      "Epoch 568/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.7014 - accuracy: 0.7737 - val_loss: 0.8814 - val_accuracy: 0.7273\n",
      "Epoch 569/15000\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.6958 - accuracy: 0.7778 - val_loss: 0.8769 - val_accuracy: 0.7273\n",
      "Epoch 570/15000\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.6895 - accuracy: 0.7798 - val_loss: 0.8739 - val_accuracy: 0.7455\n",
      "Epoch 571/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.6867 - accuracy: 0.7840 - val_loss: 0.8689 - val_accuracy: 0.7455\n",
      "Epoch 572/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.6820 - accuracy: 0.7984 - val_loss: 0.8664 - val_accuracy: 0.7636\n",
      "Epoch 573/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.6761 - accuracy: 0.7984 - val_loss: 0.8607 - val_accuracy: 0.7636\n",
      "Epoch 574/15000\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.6725 - accuracy: 0.8004 - val_loss: 0.8573 - val_accuracy: 0.7636\n",
      "Epoch 575/15000\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 0.6679 - accuracy: 0.8004 - val_loss: 0.8550 - val_accuracy: 0.7636\n",
      "Epoch 576/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.6638 - accuracy: 0.7984 - val_loss: 0.8511 - val_accuracy: 0.7636\n",
      "Epoch 577/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.6595 - accuracy: 0.7942 - val_loss: 0.8494 - val_accuracy: 0.7455\n",
      "Epoch 578/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.6556 - accuracy: 0.7984 - val_loss: 0.8452 - val_accuracy: 0.7636\n",
      "Epoch 579/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.6518 - accuracy: 0.8025 - val_loss: 0.8410 - val_accuracy: 0.7455\n",
      "Epoch 580/15000\n",
      "16/16 [==============================] - 3s 221ms/step - loss: 0.6469 - accuracy: 0.8066 - val_loss: 0.8385 - val_accuracy: 0.7455\n",
      "Epoch 581/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.6439 - accuracy: 0.8107 - val_loss: 0.8347 - val_accuracy: 0.7455\n",
      "Epoch 582/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.6415 - accuracy: 0.8107 - val_loss: 0.8328 - val_accuracy: 0.7455\n",
      "Epoch 583/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.6367 - accuracy: 0.8128 - val_loss: 0.8311 - val_accuracy: 0.7818\n",
      "Epoch 584/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.6339 - accuracy: 0.8107 - val_loss: 0.8268 - val_accuracy: 0.7636\n",
      "Epoch 585/15000\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.6287 - accuracy: 0.8107 - val_loss: 0.8237 - val_accuracy: 0.7636\n",
      "Epoch 586/15000\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 0.6271 - accuracy: 0.8086 - val_loss: 0.8197 - val_accuracy: 0.7636\n",
      "Epoch 587/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.6215 - accuracy: 0.8148 - val_loss: 0.8166 - val_accuracy: 0.7818\n",
      "Epoch 588/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.6181 - accuracy: 0.8128 - val_loss: 0.8143 - val_accuracy: 0.7818\n",
      "Epoch 589/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.6171 - accuracy: 0.8128 - val_loss: 0.8110 - val_accuracy: 0.7636\n",
      "Epoch 590/15000\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.6133 - accuracy: 0.8169 - val_loss: 0.8098 - val_accuracy: 0.7818\n",
      "Epoch 591/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.6120 - accuracy: 0.8169 - val_loss: 0.8055 - val_accuracy: 0.7636\n",
      "Epoch 592/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.6067 - accuracy: 0.8189 - val_loss: 0.8014 - val_accuracy: 0.7636\n",
      "Epoch 593/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.6040 - accuracy: 0.8128 - val_loss: 0.7995 - val_accuracy: 0.7455\n",
      "Epoch 594/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.6016 - accuracy: 0.8148 - val_loss: 0.7973 - val_accuracy: 0.7636\n",
      "Epoch 595/15000\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.5982 - accuracy: 0.8169 - val_loss: 0.7943 - val_accuracy: 0.7818\n",
      "Epoch 596/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5954 - accuracy: 0.8251 - val_loss: 0.7915 - val_accuracy: 0.7818\n",
      "Epoch 597/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5925 - accuracy: 0.8272 - val_loss: 0.7883 - val_accuracy: 0.7636\n",
      "Epoch 598/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.5895 - accuracy: 0.8251 - val_loss: 0.7860 - val_accuracy: 0.7636\n",
      "Epoch 599/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5865 - accuracy: 0.8272 - val_loss: 0.7831 - val_accuracy: 0.7636\n",
      "Epoch 600/15000\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.5839 - accuracy: 0.8272 - val_loss: 0.7814 - val_accuracy: 0.7636\n",
      "Epoch 601/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5826 - accuracy: 0.8272 - val_loss: 0.7758 - val_accuracy: 0.7818\n",
      "Epoch 602/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5789 - accuracy: 0.8333 - val_loss: 0.7723 - val_accuracy: 0.7818\n",
      "Epoch 603/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5756 - accuracy: 0.8333 - val_loss: 0.7693 - val_accuracy: 0.7818\n",
      "Epoch 604/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5728 - accuracy: 0.8313 - val_loss: 0.7653 - val_accuracy: 0.7818\n",
      "Epoch 605/15000\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.5712 - accuracy: 0.8272 - val_loss: 0.7632 - val_accuracy: 0.8000\n",
      "Epoch 606/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5680 - accuracy: 0.8457 - val_loss: 0.7589 - val_accuracy: 0.8000\n",
      "Epoch 607/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5652 - accuracy: 0.8416 - val_loss: 0.7559 - val_accuracy: 0.8000\n",
      "Epoch 608/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5626 - accuracy: 0.8416 - val_loss: 0.7539 - val_accuracy: 0.8000\n",
      "Epoch 609/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5610 - accuracy: 0.8416 - val_loss: 0.7505 - val_accuracy: 0.7818\n",
      "Epoch 610/15000\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.5578 - accuracy: 0.8416 - val_loss: 0.7482 - val_accuracy: 0.7818\n",
      "Epoch 611/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5549 - accuracy: 0.8416 - val_loss: 0.7472 - val_accuracy: 0.8000\n",
      "Epoch 612/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5531 - accuracy: 0.8457 - val_loss: 0.7436 - val_accuracy: 0.8000\n",
      "Epoch 613/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5504 - accuracy: 0.8436 - val_loss: 0.7415 - val_accuracy: 0.8000\n",
      "Epoch 614/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5470 - accuracy: 0.8498 - val_loss: 0.7387 - val_accuracy: 0.7818\n",
      "Epoch 615/15000\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.5451 - accuracy: 0.8539 - val_loss: 0.7352 - val_accuracy: 0.8182\n",
      "Epoch 616/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5421 - accuracy: 0.8580 - val_loss: 0.7327 - val_accuracy: 0.8182\n",
      "Epoch 617/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5391 - accuracy: 0.8519 - val_loss: 0.7281 - val_accuracy: 0.8000\n",
      "Epoch 618/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.5390 - accuracy: 0.8477 - val_loss: 0.7247 - val_accuracy: 0.8000\n",
      "Epoch 619/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5349 - accuracy: 0.8498 - val_loss: 0.7210 - val_accuracy: 0.8000\n",
      "Epoch 620/15000\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.5329 - accuracy: 0.8498 - val_loss: 0.7188 - val_accuracy: 0.8000\n",
      "Epoch 621/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.5296 - accuracy: 0.8519 - val_loss: 0.7161 - val_accuracy: 0.8182\n",
      "Epoch 622/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5267 - accuracy: 0.8621 - val_loss: 0.7116 - val_accuracy: 0.8182\n",
      "Epoch 623/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.5248 - accuracy: 0.8621 - val_loss: 0.7070 - val_accuracy: 0.8364\n",
      "Epoch 624/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.5222 - accuracy: 0.8642 - val_loss: 0.7058 - val_accuracy: 0.8364\n",
      "Epoch 625/15000\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.5201 - accuracy: 0.8621 - val_loss: 0.7010 - val_accuracy: 0.8364\n",
      "Epoch 626/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.5173 - accuracy: 0.8683 - val_loss: 0.7002 - val_accuracy: 0.8364\n",
      "Epoch 627/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5152 - accuracy: 0.8683 - val_loss: 0.6977 - val_accuracy: 0.8364\n",
      "Epoch 628/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5125 - accuracy: 0.8642 - val_loss: 0.6922 - val_accuracy: 0.8364\n",
      "Epoch 629/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.5105 - accuracy: 0.8683 - val_loss: 0.6893 - val_accuracy: 0.8364\n",
      "Epoch 630/15000\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.5080 - accuracy: 0.8704 - val_loss: 0.6882 - val_accuracy: 0.8364\n",
      "Epoch 631/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.5054 - accuracy: 0.8663 - val_loss: 0.6860 - val_accuracy: 0.8545\n",
      "Epoch 632/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5029 - accuracy: 0.8745 - val_loss: 0.6828 - val_accuracy: 0.8545\n",
      "Epoch 633/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.5007 - accuracy: 0.8745 - val_loss: 0.6779 - val_accuracy: 0.8545\n",
      "Epoch 634/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.4987 - accuracy: 0.8807 - val_loss: 0.6762 - val_accuracy: 0.8545\n",
      "Epoch 635/15000\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.4960 - accuracy: 0.8807 - val_loss: 0.6735 - val_accuracy: 0.8364\n",
      "Epoch 636/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4938 - accuracy: 0.8745 - val_loss: 0.6718 - val_accuracy: 0.8364\n",
      "Epoch 637/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4913 - accuracy: 0.8807 - val_loss: 0.6695 - val_accuracy: 0.8545\n",
      "Epoch 638/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4896 - accuracy: 0.8786 - val_loss: 0.6688 - val_accuracy: 0.8545\n",
      "Epoch 639/15000\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.4869 - accuracy: 0.8786 - val_loss: 0.6657 - val_accuracy: 0.8364\n",
      "Epoch 640/15000\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.4851 - accuracy: 0.8765 - val_loss: 0.6618 - val_accuracy: 0.8364\n",
      "Epoch 641/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4827 - accuracy: 0.8807 - val_loss: 0.6608 - val_accuracy: 0.8364\n",
      "Epoch 642/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4802 - accuracy: 0.8807 - val_loss: 0.6601 - val_accuracy: 0.8364\n",
      "Epoch 643/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4785 - accuracy: 0.8745 - val_loss: 0.6542 - val_accuracy: 0.8545\n",
      "Epoch 644/15000\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.4761 - accuracy: 0.8786 - val_loss: 0.6510 - val_accuracy: 0.8545\n",
      "Epoch 645/15000\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.4744 - accuracy: 0.8848 - val_loss: 0.6452 - val_accuracy: 0.8364\n",
      "Epoch 646/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4715 - accuracy: 0.8868 - val_loss: 0.6444 - val_accuracy: 0.8545\n",
      "Epoch 647/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4694 - accuracy: 0.8848 - val_loss: 0.6433 - val_accuracy: 0.8545\n",
      "Epoch 648/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4673 - accuracy: 0.8889 - val_loss: 0.6351 - val_accuracy: 0.8545\n",
      "Epoch 649/15000\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.4654 - accuracy: 0.8909 - val_loss: 0.6318 - val_accuracy: 0.8545\n",
      "Epoch 650/15000\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 0.4633 - accuracy: 0.8951 - val_loss: 0.6304 - val_accuracy: 0.8545\n",
      "Epoch 651/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.4610 - accuracy: 0.8909 - val_loss: 0.6312 - val_accuracy: 0.8545\n",
      "Epoch 652/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.4589 - accuracy: 0.8951 - val_loss: 0.6269 - val_accuracy: 0.8545\n",
      "Epoch 653/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4566 - accuracy: 0.8889 - val_loss: 0.6249 - val_accuracy: 0.8545\n",
      "Epoch 654/15000\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.4543 - accuracy: 0.8868 - val_loss: 0.6229 - val_accuracy: 0.8545\n",
      "Epoch 655/15000\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.4518 - accuracy: 0.8889 - val_loss: 0.6190 - val_accuracy: 0.8545\n",
      "Epoch 656/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4497 - accuracy: 0.8930 - val_loss: 0.6175 - val_accuracy: 0.8545\n",
      "Epoch 657/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.4479 - accuracy: 0.8909 - val_loss: 0.6137 - val_accuracy: 0.8545\n",
      "Epoch 658/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4460 - accuracy: 0.8992 - val_loss: 0.6126 - val_accuracy: 0.8545\n",
      "Epoch 659/15000\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.4437 - accuracy: 0.8971 - val_loss: 0.6031 - val_accuracy: 0.8545\n",
      "Epoch 660/15000\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.4418 - accuracy: 0.8971 - val_loss: 0.5947 - val_accuracy: 0.8727\n",
      "Epoch 661/15000\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 0.4403 - accuracy: 0.8951 - val_loss: 0.5913 - val_accuracy: 0.8727\n",
      "Epoch 662/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.4376 - accuracy: 0.8971 - val_loss: 0.5924 - val_accuracy: 0.8545\n",
      "Epoch 663/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4351 - accuracy: 0.9012 - val_loss: 0.5927 - val_accuracy: 0.8545\n",
      "Epoch 664/15000\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.4326 - accuracy: 0.9012 - val_loss: 0.5885 - val_accuracy: 0.8545\n",
      "Epoch 665/15000\n",
      "16/16 [==============================] - 4s 217ms/step - loss: 0.4303 - accuracy: 0.9033 - val_loss: 0.5874 - val_accuracy: 0.8545\n",
      "Epoch 666/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4286 - accuracy: 0.9033 - val_loss: 0.5850 - val_accuracy: 0.8727\n",
      "Epoch 667/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4303 - accuracy: 0.9033 - val_loss: 0.5834 - val_accuracy: 0.8727\n",
      "Epoch 668/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.4245 - accuracy: 0.9095 - val_loss: 0.5780 - val_accuracy: 0.8727\n",
      "Epoch 669/15000\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.4217 - accuracy: 0.9095 - val_loss: 0.5796 - val_accuracy: 0.8545\n",
      "Epoch 670/15000\n",
      "16/16 [==============================] - 3s 202ms/step - loss: 0.4194 - accuracy: 0.9074 - val_loss: 0.5744 - val_accuracy: 0.8545\n",
      "Epoch 671/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4170 - accuracy: 0.9053 - val_loss: 0.5688 - val_accuracy: 0.8727\n",
      "Epoch 672/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.4144 - accuracy: 0.9053 - val_loss: 0.5663 - val_accuracy: 0.8545\n",
      "Epoch 673/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.4127 - accuracy: 0.9033 - val_loss: 0.5627 - val_accuracy: 0.8727\n",
      "Epoch 674/15000\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 0.4112 - accuracy: 0.9074 - val_loss: 0.5625 - val_accuracy: 0.8545\n",
      "Epoch 675/15000\n",
      "16/16 [==============================] - 3s 189ms/step - loss: 0.4084 - accuracy: 0.9074 - val_loss: 0.5599 - val_accuracy: 0.8727\n",
      "Epoch 676/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.4063 - accuracy: 0.9074 - val_loss: 0.5559 - val_accuracy: 0.8727\n",
      "Epoch 677/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4048 - accuracy: 0.9053 - val_loss: 0.5525 - val_accuracy: 0.8727\n",
      "Epoch 678/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4031 - accuracy: 0.9074 - val_loss: 0.5523 - val_accuracy: 0.8727\n",
      "Epoch 679/15000\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 0.4004 - accuracy: 0.9074 - val_loss: 0.5485 - val_accuracy: 0.8727\n",
      "Epoch 680/15000\n",
      "16/16 [==============================] - 3s 192ms/step - loss: 0.3983 - accuracy: 0.9095 - val_loss: 0.5441 - val_accuracy: 0.8727\n",
      "Epoch 681/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.3960 - accuracy: 0.9053 - val_loss: 0.5411 - val_accuracy: 0.8727\n",
      "Epoch 682/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.9095\n",
      "Yes You are here 0.9094650149345398 0.8909090757369995\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.3940 - accuracy: 0.9095 - val_loss: 0.5368 - val_accuracy: 0.8909\n",
      "Epoch 683/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3918 - accuracy: 0.9095 - val_loss: 0.5357 - val_accuracy: 0.8727\n",
      "Epoch 684/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3906 - accuracy: 0.9095\n",
      "Yes You are here 0.9094650149345398 0.8909090757369995\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.3906 - accuracy: 0.9095 - val_loss: 0.5336 - val_accuracy: 0.8909\n",
      "Epoch 685/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.9095\n",
      "Yes You are here 0.9094650149345398 0.8909090757369995\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.3883 - accuracy: 0.9095 - val_loss: 0.5284 - val_accuracy: 0.8909\n",
      "Epoch 686/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.9095\n",
      "Yes You are here 0.9094650149345398 0.8909090757369995\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3865 - accuracy: 0.9095 - val_loss: 0.5227 - val_accuracy: 0.8909\n",
      "Epoch 687/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.9074\n",
      "Yes You are here 0.9074074029922485 0.8909090757369995\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3845 - accuracy: 0.9074 - val_loss: 0.5211 - val_accuracy: 0.8909\n",
      "Epoch 688/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.9095\n",
      "Yes You are here 0.9094650149345398 0.8909090757369995\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.3830 - accuracy: 0.9095 - val_loss: 0.5172 - val_accuracy: 0.8909\n",
      "Epoch 689/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.9115\n",
      "Yes You are here 0.911522626876831 0.8909090757369995\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.3803 - accuracy: 0.9115 - val_loss: 0.5136 - val_accuracy: 0.8909\n",
      "Epoch 690/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3790 - accuracy: 0.9074\n",
      "Yes You are here 0.9074074029922485 0.8909090757369995\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.3790 - accuracy: 0.9074 - val_loss: 0.5126 - val_accuracy: 0.8909\n",
      "Epoch 691/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3767 - accuracy: 0.9053\n",
      "Yes You are here 0.9053497910499573 0.8909090757369995\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3767 - accuracy: 0.9053 - val_loss: 0.5091 - val_accuracy: 0.8909\n",
      "Epoch 692/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3750 - accuracy: 0.9115\n",
      "Yes You are here 0.911522626876831 0.8909090757369995\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.3750 - accuracy: 0.9115 - val_loss: 0.5083 - val_accuracy: 0.8909\n",
      "Epoch 693/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3733 - accuracy: 0.9115\n",
      "Yes You are here 0.911522626876831 0.8909090757369995\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.3733 - accuracy: 0.9115 - val_loss: 0.5049 - val_accuracy: 0.8909\n",
      "Epoch 694/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3713 - accuracy: 0.9095\n",
      "Yes You are here 0.9094650149345398 0.8909090757369995\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.3713 - accuracy: 0.9095 - val_loss: 0.5013 - val_accuracy: 0.8909\n",
      "Epoch 695/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.7397 - accuracy: 0.8889 - val_loss: 1.6440 - val_accuracy: 0.8364\n",
      "Epoch 696/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 1.5386 - accuracy: 0.8313 - val_loss: 2.5916 - val_accuracy: 0.8364\n",
      "Epoch 697/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 1.0261 - accuracy: 0.8313 - val_loss: 1.0406 - val_accuracy: 0.8364\n",
      "Epoch 698/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.5458 - accuracy: 0.8704 - val_loss: 0.7543 - val_accuracy: 0.8545\n",
      "Epoch 699/15000\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 1.7804 - accuracy: 0.8642 - val_loss: 1.4138 - val_accuracy: 0.8545\n",
      "Epoch 700/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.8572 - accuracy: 0.8683 - val_loss: 0.7785 - val_accuracy: 0.8727\n",
      "Epoch 701/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.6349 - accuracy: 0.8745 - val_loss: 0.5521 - val_accuracy: 0.8727\n",
      "Epoch 702/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.5347 - accuracy: 0.8765 - val_loss: 0.5354 - val_accuracy: 0.8364\n",
      "Epoch 703/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.4706 - accuracy: 0.8765 - val_loss: 0.5376 - val_accuracy: 0.8364\n",
      "Epoch 704/15000\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.5682 - accuracy: 0.8745 - val_loss: 0.5323 - val_accuracy: 0.8727\n",
      "Epoch 705/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 1.6385 - accuracy: 0.8663 - val_loss: 0.5905 - val_accuracy: 0.8727\n",
      "Epoch 706/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.8328 - accuracy: 0.8724 - val_loss: 0.7747 - val_accuracy: 0.8182\n",
      "Epoch 707/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.6271 - accuracy: 0.8807 - val_loss: 0.6384 - val_accuracy: 0.8182\n",
      "Epoch 708/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4436 - accuracy: 0.8663 - val_loss: 0.5341 - val_accuracy: 0.8364\n",
      "Epoch 709/15000\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.4437 - accuracy: 0.8704 - val_loss: 0.5164 - val_accuracy: 0.8364\n",
      "Epoch 710/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.4347 - accuracy: 0.8745 - val_loss: 0.5581 - val_accuracy: 0.8182\n",
      "Epoch 711/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4332 - accuracy: 0.8848 - val_loss: 0.5138 - val_accuracy: 0.8545\n",
      "Epoch 712/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.4243 - accuracy: 0.8909 - val_loss: 0.5132 - val_accuracy: 0.8545\n",
      "Epoch 713/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4204 - accuracy: 0.8848 - val_loss: 0.5149 - val_accuracy: 0.8364\n",
      "Epoch 714/15000\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.4164 - accuracy: 0.8909 - val_loss: 0.5089 - val_accuracy: 0.8364\n",
      "Epoch 715/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4124 - accuracy: 0.8951 - val_loss: 0.5118 - val_accuracy: 0.8545\n",
      "Epoch 716/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.4097 - accuracy: 0.8951 - val_loss: 0.4949 - val_accuracy: 0.8727\n",
      "Epoch 717/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4082 - accuracy: 0.8971 - val_loss: 0.5122 - val_accuracy: 0.8364\n",
      "Epoch 718/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.4026 - accuracy: 0.8992 - val_loss: 0.4813 - val_accuracy: 0.8545\n",
      "Epoch 719/15000\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.4062 - accuracy: 0.9012 - val_loss: 0.4922 - val_accuracy: 0.8545\n",
      "Epoch 720/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3990 - accuracy: 0.9012 - val_loss: 0.4784 - val_accuracy: 0.8727\n",
      "Epoch 721/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.3984 - accuracy: 0.8951 - val_loss: 0.4806 - val_accuracy: 0.8727\n",
      "Epoch 722/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3924 - accuracy: 0.8992 - val_loss: 0.4771 - val_accuracy: 0.8727\n",
      "Epoch 723/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.3894 - accuracy: 0.8951 - val_loss: 0.4749 - val_accuracy: 0.8545\n",
      "Epoch 724/15000\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.3893 - accuracy: 0.8971 - val_loss: 0.4911 - val_accuracy: 0.8545\n",
      "Epoch 725/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.3849 - accuracy: 0.9053 - val_loss: 0.4813 - val_accuracy: 0.8545\n",
      "Epoch 726/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.3828 - accuracy: 0.9012 - val_loss: 0.4601 - val_accuracy: 0.8727\n",
      "Epoch 727/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.3797 - accuracy: 0.9012 - val_loss: 0.4623 - val_accuracy: 0.8727\n",
      "Epoch 728/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.3755 - accuracy: 0.9033 - val_loss: 0.4608 - val_accuracy: 0.8727\n",
      "Epoch 729/15000\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.3748 - accuracy: 0.8992 - val_loss: 0.4792 - val_accuracy: 0.8545\n",
      "Epoch 730/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3743 - accuracy: 0.9053 - val_loss: 0.4465 - val_accuracy: 0.8727\n",
      "Epoch 731/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3713 - accuracy: 0.9074 - val_loss: 0.4590 - val_accuracy: 0.8545\n",
      "Epoch 732/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.3753 - accuracy: 0.9053 - val_loss: 0.5328 - val_accuracy: 0.8727\n",
      "Epoch 733/15000\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.4143 - accuracy: 0.9033 - val_loss: 0.5637 - val_accuracy: 0.8545\n",
      "Epoch 734/15000\n",
      "16/16 [==============================] - 3s 209ms/step - loss: 0.4169 - accuracy: 0.8992 - val_loss: 0.5488 - val_accuracy: 0.8545\n",
      "Epoch 735/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.3970 - accuracy: 0.9012 - val_loss: 0.5570 - val_accuracy: 0.8182\n",
      "Epoch 736/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4012 - accuracy: 0.9074 - val_loss: 0.5458 - val_accuracy: 0.8364\n",
      "Epoch 737/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.3909 - accuracy: 0.9074 - val_loss: 0.5273 - val_accuracy: 0.8545\n",
      "Epoch 738/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.9136\n",
      "Yes You are here 0.9135802388191223 0.8909090757369995\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.3826 - accuracy: 0.9136 - val_loss: 0.4998 - val_accuracy: 0.8909\n",
      "Epoch 739/15000\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.3809 - accuracy: 0.9177 - val_loss: 0.4961 - val_accuracy: 0.8727\n",
      "Epoch 740/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3775 - accuracy: 0.9136 - val_loss: 0.4840 - val_accuracy: 0.8727\n",
      "Epoch 741/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.3834 - accuracy: 0.9156 - val_loss: 0.4824 - val_accuracy: 0.8545\n",
      "Epoch 742/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.3620 - accuracy: 0.9177 - val_loss: 0.4854 - val_accuracy: 0.8545\n",
      "Epoch 743/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.3623 - accuracy: 0.9177 - val_loss: 0.4717 - val_accuracy: 0.8545\n",
      "Epoch 744/15000\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.3530 - accuracy: 0.9198 - val_loss: 0.4888 - val_accuracy: 0.8364\n",
      "Epoch 745/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3508 - accuracy: 0.9198 - val_loss: 0.4731 - val_accuracy: 0.8545\n",
      "Epoch 746/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.3478 - accuracy: 0.9198 - val_loss: 0.4737 - val_accuracy: 0.8545\n",
      "Epoch 747/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.3444 - accuracy: 0.9198 - val_loss: 0.4778 - val_accuracy: 0.8364\n",
      "Epoch 748/15000\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.3467 - accuracy: 0.9156 - val_loss: 0.4670 - val_accuracy: 0.8545\n",
      "Epoch 749/15000\n",
      "16/16 [==============================] - 4s 218ms/step - loss: 0.3546 - accuracy: 0.9198 - val_loss: 0.4701 - val_accuracy: 0.8727\n",
      "Epoch 750/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.5892 - accuracy: 0.9033 - val_loss: 0.4613 - val_accuracy: 0.8545\n",
      "Epoch 751/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3626 - accuracy: 0.9156 - val_loss: 0.5094 - val_accuracy: 0.8364\n",
      "Epoch 752/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.3419 - accuracy: 0.9136 - val_loss: 0.4608 - val_accuracy: 0.8545\n",
      "Epoch 753/15000\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.3455 - accuracy: 0.9033 - val_loss: 0.4704 - val_accuracy: 0.8364\n",
      "Epoch 754/15000\n",
      "16/16 [==============================] - 3s 206ms/step - loss: 0.3366 - accuracy: 0.9156 - val_loss: 0.4623 - val_accuracy: 0.8364\n",
      "Epoch 755/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.3351 - accuracy: 0.9156 - val_loss: 0.4532 - val_accuracy: 0.8545\n",
      "Epoch 756/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3300 - accuracy: 0.9198 - val_loss: 0.4615 - val_accuracy: 0.8364\n",
      "Epoch 757/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.3250 - accuracy: 0.9239 - val_loss: 0.4488 - val_accuracy: 0.8545\n",
      "Epoch 758/15000\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.3235 - accuracy: 0.9218 - val_loss: 0.4462 - val_accuracy: 0.8727\n",
      "Epoch 759/15000\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 0.3193 - accuracy: 0.9259 - val_loss: 0.4537 - val_accuracy: 0.8545\n",
      "Epoch 760/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.3180 - accuracy: 0.9259 - val_loss: 0.4444 - val_accuracy: 0.8727\n",
      "Epoch 761/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.9259\n",
      "Yes You are here 0.9259259104728699 0.8909090757369995\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.3148 - accuracy: 0.9259 - val_loss: 0.4378 - val_accuracy: 0.8909\n",
      "Epoch 762/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.3132 - accuracy: 0.9259 - val_loss: 0.4414 - val_accuracy: 0.8545\n",
      "Epoch 763/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.9259\n",
      "Yes You are here 0.9259259104728699 0.8909090757369995\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.3120 - accuracy: 0.9259 - val_loss: 0.4362 - val_accuracy: 0.8909\n",
      "Epoch 764/15000\n",
      "16/16 [==============================] - 3s 191ms/step - loss: 0.3094 - accuracy: 0.9259 - val_loss: 0.4366 - val_accuracy: 0.8727\n",
      "Epoch 765/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.3071 - accuracy: 0.9259 - val_loss: 0.4358 - val_accuracy: 0.8727\n",
      "Epoch 766/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.3051 - accuracy: 0.9239 - val_loss: 0.4330 - val_accuracy: 0.8545\n",
      "Epoch 767/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.3022 - accuracy: 0.9259 - val_loss: 0.4359 - val_accuracy: 0.8727\n",
      "Epoch 768/15000\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.3074 - accuracy: 0.9259 - val_loss: 0.4453 - val_accuracy: 0.8727\n",
      "Epoch 769/15000\n",
      "16/16 [==============================] - 3s 200ms/step - loss: 0.3928 - accuracy: 0.9198 - val_loss: 0.4281 - val_accuracy: 0.8727\n",
      "Epoch 770/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.3400 - accuracy: 0.9239 - val_loss: 0.4518 - val_accuracy: 0.8727\n",
      "Epoch 771/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.3359 - accuracy: 0.9239 - val_loss: 0.4798 - val_accuracy: 0.8545\n",
      "Epoch 772/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.9239\n",
      "Yes You are here 0.9238682985305786 0.8909090757369995\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.3319 - accuracy: 0.9239 - val_loss: 0.4316 - val_accuracy: 0.8909\n",
      "Epoch 773/15000\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.3106 - accuracy: 0.9280 - val_loss: 0.4223 - val_accuracy: 0.8727\n",
      "Epoch 774/15000\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.3006 - accuracy: 0.9321 - val_loss: 0.4305 - val_accuracy: 0.8727\n",
      "Epoch 775/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.8909090757369995\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2981 - accuracy: 0.9342 - val_loss: 0.4162 - val_accuracy: 0.8909\n",
      "Epoch 776/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.8909090757369995\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2942 - accuracy: 0.9342 - val_loss: 0.4171 - val_accuracy: 0.8909\n",
      "Epoch 777/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2935 - accuracy: 0.9362 - val_loss: 0.4121 - val_accuracy: 0.9091\n",
      "Epoch 778/15000\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.2903 - accuracy: 0.9362 - val_loss: 0.4140 - val_accuracy: 0.8909\n",
      "Epoch 779/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 3s 155ms/step - loss: 0.2877 - accuracy: 0.9362 - val_loss: 0.4075 - val_accuracy: 0.9091\n",
      "Epoch 780/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2859 - accuracy: 0.9321 - val_loss: 0.4102 - val_accuracy: 0.8727\n",
      "Epoch 781/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.2847 - accuracy: 0.9300 - val_loss: 0.4042 - val_accuracy: 0.8909\n",
      "Epoch 782/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2822 - accuracy: 0.9321 - val_loss: 0.4038 - val_accuracy: 0.8909\n",
      "Epoch 783/15000\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.2796 - accuracy: 0.9321 - val_loss: 0.4022 - val_accuracy: 0.8909\n",
      "Epoch 784/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.2783 - accuracy: 0.9362 - val_loss: 0.4019 - val_accuracy: 0.9091\n",
      "Epoch 785/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.2762 - accuracy: 0.9300 - val_loss: 0.4043 - val_accuracy: 0.8727\n",
      "Epoch 786/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.2737 - accuracy: 0.9280 - val_loss: 0.3992 - val_accuracy: 0.8909\n",
      "Epoch 787/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.2725 - accuracy: 0.9280 - val_loss: 0.3977 - val_accuracy: 0.8909\n",
      "Epoch 788/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.9090909361839294\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.2714 - accuracy: 0.9342 - val_loss: 0.3936 - val_accuracy: 0.9091\n",
      "Epoch 789/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2686 - accuracy: 0.9321 - val_loss: 0.3893 - val_accuracy: 0.8909\n",
      "Epoch 790/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.2665 - accuracy: 0.9259 - val_loss: 0.3980 - val_accuracy: 0.8727\n",
      "Epoch 791/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9280\n",
      "Yes You are here 0.9279835224151611 0.9090909361839294\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.2658 - accuracy: 0.9280 - val_loss: 0.3874 - val_accuracy: 0.9091\n",
      "Epoch 792/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9300\n",
      "Yes You are here 0.9300411343574524 0.9090909361839294\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.2646 - accuracy: 0.9300 - val_loss: 0.3875 - val_accuracy: 0.9091\n",
      "Epoch 793/15000\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.2627 - accuracy: 0.9259 - val_loss: 0.3862 - val_accuracy: 0.8909\n",
      "Epoch 794/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.2602 - accuracy: 0.9280 - val_loss: 0.3896 - val_accuracy: 0.8727\n",
      "Epoch 795/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2593 - accuracy: 0.9321 - val_loss: 0.3844 - val_accuracy: 0.8909\n",
      "Epoch 796/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.2566 - accuracy: 0.9300 - val_loss: 0.3757 - val_accuracy: 0.8909\n",
      "Epoch 797/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.2569 - accuracy: 0.9259 - val_loss: 0.3848 - val_accuracy: 0.8727\n",
      "Epoch 798/15000\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.2550 - accuracy: 0.9300 - val_loss: 0.3759 - val_accuracy: 0.8909\n",
      "Epoch 799/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2539 - accuracy: 0.9300 - val_loss: 0.3723 - val_accuracy: 0.8909\n",
      "Epoch 800/15000\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.2514 - accuracy: 0.9321 - val_loss: 0.3720 - val_accuracy: 0.8909\n",
      "Epoch 801/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2499 - accuracy: 0.9300 - val_loss: 0.3687 - val_accuracy: 0.8909\n",
      "Epoch 802/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9300\n",
      "Yes You are here 0.9300411343574524 0.9090909361839294\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2478 - accuracy: 0.9300 - val_loss: 0.3660 - val_accuracy: 0.9091\n",
      "Epoch 803/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.9090909361839294\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.2460 - accuracy: 0.9342 - val_loss: 0.3658 - val_accuracy: 0.9091\n",
      "Epoch 804/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.9090909361839294\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2441 - accuracy: 0.9342 - val_loss: 0.3647 - val_accuracy: 0.9091\n",
      "Epoch 805/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9321\n",
      "Yes You are here 0.9320987462997437 0.9090909361839294\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2436 - accuracy: 0.9321 - val_loss: 0.3636 - val_accuracy: 0.9091\n",
      "Epoch 806/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2417 - accuracy: 0.9362 - val_loss: 0.3618 - val_accuracy: 0.9091\n",
      "Epoch 807/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2416 - accuracy: 0.9362 - val_loss: 0.3570 - val_accuracy: 0.9091\n",
      "Epoch 808/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.9090909361839294\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.2391 - accuracy: 0.9342 - val_loss: 0.3581 - val_accuracy: 0.9091\n",
      "Epoch 809/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.2377 - accuracy: 0.9362 - val_loss: 0.3573 - val_accuracy: 0.9091\n",
      "Epoch 810/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9321\n",
      "Yes You are here 0.9320987462997437 0.9090909361839294\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.2363 - accuracy: 0.9321 - val_loss: 0.3562 - val_accuracy: 0.9091\n",
      "Epoch 811/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.9090909361839294\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.2333 - accuracy: 0.9342 - val_loss: 0.3535 - val_accuracy: 0.9091\n",
      "Epoch 812/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.2333 - accuracy: 0.9362 - val_loss: 0.3528 - val_accuracy: 0.9091\n",
      "Epoch 813/15000\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.2313 - accuracy: 0.9383 - val_loss: 0.3515 - val_accuracy: 0.8909\n",
      "Epoch 814/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9383\n",
      "Yes You are here 0.9382715821266174 0.9090909361839294\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2293 - accuracy: 0.9383 - val_loss: 0.3490 - val_accuracy: 0.9091\n",
      "Epoch 815/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2286 - accuracy: 0.9362 - val_loss: 0.3414 - val_accuracy: 0.9091\n",
      "Epoch 816/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.2277 - accuracy: 0.9362 - val_loss: 0.3441 - val_accuracy: 0.9091\n",
      "Epoch 817/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9383\n",
      "Yes You are here 0.9382715821266174 0.9090909361839294\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.2253 - accuracy: 0.9383 - val_loss: 0.3400 - val_accuracy: 0.9091\n",
      "Epoch 818/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.2250 - accuracy: 0.9362 - val_loss: 0.3385 - val_accuracy: 0.9091\n",
      "Epoch 819/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9486\n",
      "Yes You are here 0.9485596418380737 0.9090909361839294\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.2242 - accuracy: 0.9486 - val_loss: 0.3354 - val_accuracy: 0.9091\n",
      "Epoch 820/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9383\n",
      "Yes You are here 0.9382715821266174 0.9090909361839294\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.2219 - accuracy: 0.9383 - val_loss: 0.3372 - val_accuracy: 0.9091\n",
      "Epoch 821/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9444\n",
      "Yes You are here 0.9444444179534912 0.9090909361839294\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.2207 - accuracy: 0.9444 - val_loss: 0.3320 - val_accuracy: 0.9091\n",
      "Epoch 822/15000\n",
      "16/16 [==============================] - 3s 199ms/step - loss: 0.2205 - accuracy: 0.9342 - val_loss: 0.3367 - val_accuracy: 0.8909\n",
      "Epoch 823/15000\n",
      "16/16 [==============================] - 3s 191ms/step - loss: 0.2183 - accuracy: 0.9321 - val_loss: 0.3305 - val_accuracy: 0.8909\n",
      "Epoch 824/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9362\n",
      "Yes You are here 0.9362139701843262 0.9090909361839294\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.2195 - accuracy: 0.9362 - val_loss: 0.3296 - val_accuracy: 0.9091\n",
      "Epoch 825/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.2193 - accuracy: 0.9342 - val_loss: 0.3270 - val_accuracy: 0.8909\n",
      "Epoch 826/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.2232 - accuracy: 0.9259 - val_loss: 0.3519 - val_accuracy: 0.8727\n",
      "Epoch 827/15000\n",
      "16/16 [==============================] - 3s 201ms/step - loss: 0.2195 - accuracy: 0.9300 - val_loss: 0.3229 - val_accuracy: 0.8909\n",
      "Epoch 828/15000\n",
      "16/16 [==============================] - 3s 191ms/step - loss: 0.2139 - accuracy: 0.9321 - val_loss: 0.3328 - val_accuracy: 0.8727\n",
      "Epoch 829/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2094 - accuracy: 0.9342 - val_loss: 0.3205 - val_accuracy: 0.8909\n",
      "Epoch 830/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.2119 - accuracy: 0.9342 - val_loss: 0.3174 - val_accuracy: 0.8909\n",
      "Epoch 831/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2081 - accuracy: 0.9342 - val_loss: 0.3188 - val_accuracy: 0.8909\n",
      "Epoch 832/15000\n",
      "16/16 [==============================] - 3s 215ms/step - loss: 0.2138 - accuracy: 0.9300 - val_loss: 0.4582 - val_accuracy: 0.8727\n",
      "Epoch 833/15000\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.2464 - accuracy: 0.9239 - val_loss: 0.3196 - val_accuracy: 0.8909\n",
      "Epoch 834/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.7480 - accuracy: 0.9074 - val_loss: 0.3335 - val_accuracy: 0.8909\n",
      "Epoch 835/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2766 - accuracy: 0.9095 - val_loss: 0.3343 - val_accuracy: 0.8727\n",
      "Epoch 836/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.2388 - accuracy: 0.9218 - val_loss: 0.3160 - val_accuracy: 0.8909\n",
      "Epoch 837/15000\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.2166 - accuracy: 0.9383 - val_loss: 1.2031 - val_accuracy: 0.8182\n",
      "Epoch 838/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.9136\n",
      "Yes You are here 0.9135802388191223 0.9090909361839294\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.5146 - accuracy: 0.9136 - val_loss: 0.3605 - val_accuracy: 0.9091\n",
      "Epoch 839/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2596 - accuracy: 0.9321 - val_loss: 0.3868 - val_accuracy: 0.8909\n",
      "Epoch 840/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9198\n",
      "Yes You are here 0.9197530746459961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.2914 - accuracy: 0.9198 - val_loss: 0.3367 - val_accuracy: 0.9273\n",
      "Epoch 841/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.9506\n",
      "Yes You are here 0.9506173133850098 0.9272727370262146\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.2328 - accuracy: 0.9506 - val_loss: 0.3152 - val_accuracy: 0.9273\n",
      "Epoch 842/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9527\n",
      "Yes You are here 0.952674925327301 0.9272727370262146\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.2266 - accuracy: 0.9527 - val_loss: 0.3136 - val_accuracy: 0.9273\n",
      "Epoch 843/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2207 - accuracy: 0.9486 - val_loss: 0.3169 - val_accuracy: 0.9091\n",
      "Epoch 844/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.2158 - accuracy: 0.9465 - val_loss: 0.3140 - val_accuracy: 0.9091\n",
      "Epoch 845/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.2127 - accuracy: 0.9506 - val_loss: 0.3133 - val_accuracy: 0.9091\n",
      "Epoch 846/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.2102 - accuracy: 0.9547 - val_loss: 0.3107 - val_accuracy: 0.9091\n",
      "Epoch 847/15000\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.2074 - accuracy: 0.9547 - val_loss: 0.3092 - val_accuracy: 0.9091\n",
      "Epoch 848/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2058 - accuracy: 0.9465 - val_loss: 0.3087 - val_accuracy: 0.9091\n",
      "Epoch 849/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.2034 - accuracy: 0.9486 - val_loss: 0.3074 - val_accuracy: 0.9091\n",
      "Epoch 850/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.2027 - accuracy: 0.9486 - val_loss: 0.3042 - val_accuracy: 0.9091\n",
      "Epoch 851/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.2003 - accuracy: 0.9465 - val_loss: 0.3028 - val_accuracy: 0.9091\n",
      "Epoch 852/15000\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.1982 - accuracy: 0.9568 - val_loss: 0.2973 - val_accuracy: 0.9091\n",
      "Epoch 853/15000\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.1967 - accuracy: 0.9568 - val_loss: 0.2981 - val_accuracy: 0.9091\n",
      "Epoch 854/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.1949 - accuracy: 0.9547 - val_loss: 0.2987 - val_accuracy: 0.9091\n",
      "Epoch 855/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1930 - accuracy: 0.9506 - val_loss: 0.2979 - val_accuracy: 0.9091\n",
      "Epoch 856/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.1919 - accuracy: 0.9444 - val_loss: 0.2975 - val_accuracy: 0.8909\n",
      "Epoch 857/15000\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.1910 - accuracy: 0.9444 - val_loss: 0.2937 - val_accuracy: 0.8909\n",
      "Epoch 858/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1891 - accuracy: 0.9486 - val_loss: 0.2910 - val_accuracy: 0.9091\n",
      "Epoch 859/15000\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.1878 - accuracy: 0.9527 - val_loss: 0.2891 - val_accuracy: 0.9091\n",
      "Epoch 860/15000\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.1874 - accuracy: 0.9527 - val_loss: 0.2892 - val_accuracy: 0.9091\n",
      "Epoch 861/15000\n",
      "16/16 [==============================] - 3s 206ms/step - loss: 0.1847 - accuracy: 0.9465 - val_loss: 0.2876 - val_accuracy: 0.9091\n",
      "Epoch 862/15000\n",
      "16/16 [==============================] - 3s 198ms/step - loss: 0.1837 - accuracy: 0.9568 - val_loss: 0.2840 - val_accuracy: 0.9091\n",
      "Epoch 863/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.1821 - accuracy: 0.9568 - val_loss: 0.2834 - val_accuracy: 0.9091\n",
      "Epoch 864/15000\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.1808 - accuracy: 0.9506 - val_loss: 0.2820 - val_accuracy: 0.9091\n",
      "Epoch 865/15000\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.1794 - accuracy: 0.9486 - val_loss: 0.2799 - val_accuracy: 0.9091\n",
      "Epoch 866/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9465\n",
      "Yes You are here 0.9465020298957825 0.9272727370262146\n",
      "16/16 [==============================] - 4s 282ms/step - loss: 0.1778 - accuracy: 0.9465 - val_loss: 0.2786 - val_accuracy: 0.9273\n",
      "Epoch 867/15000\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1766 - accuracy: 0.9465 - val_loss: 0.2782 - val_accuracy: 0.9091\n",
      "Epoch 868/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9527\n",
      "Yes You are here 0.952674925327301 0.9272727370262146\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.1754 - accuracy: 0.9527 - val_loss: 0.2736 - val_accuracy: 0.9273\n",
      "Epoch 869/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9486\n",
      "Yes You are here 0.9485596418380737 0.9272727370262146\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1744 - accuracy: 0.9486 - val_loss: 0.2736 - val_accuracy: 0.9273\n",
      "Epoch 870/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9486\n",
      "Yes You are here 0.9485596418380737 0.9272727370262146\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1726 - accuracy: 0.9486 - val_loss: 0.2737 - val_accuracy: 0.9273\n",
      "Epoch 871/15000\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.1719 - accuracy: 0.9527 - val_loss: 0.2685 - val_accuracy: 0.9091\n",
      "Epoch 872/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9506\n",
      "Yes You are here 0.9506173133850098 0.9272727370262146\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1705 - accuracy: 0.9506 - val_loss: 0.2677 - val_accuracy: 0.9273\n",
      "Epoch 873/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.1694 - accuracy: 0.9568 - val_loss: 0.2666 - val_accuracy: 0.9091\n",
      "Epoch 874/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9547\n",
      "Yes You are here 0.9547325372695923 0.9272727370262146\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1690 - accuracy: 0.9547 - val_loss: 0.2633 - val_accuracy: 0.9273\n",
      "Epoch 875/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9506\n",
      "Yes You are here 0.9506173133850098 0.9272727370262146\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 0.1670 - accuracy: 0.9506 - val_loss: 0.2598 - val_accuracy: 0.9273\n",
      "Epoch 876/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9527\n",
      "Yes You are here 0.952674925327301 0.9272727370262146\n",
      "16/16 [==============================] - 3s 209ms/step - loss: 0.1652 - accuracy: 0.9527 - val_loss: 0.2586 - val_accuracy: 0.9273\n",
      "Epoch 877/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9547\n",
      "Yes You are here 0.9547325372695923 0.9272727370262146\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1639 - accuracy: 0.9547 - val_loss: 0.2570 - val_accuracy: 0.9273\n",
      "Epoch 878/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9588\n",
      "Yes You are here 0.9588477611541748 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1634 - accuracy: 0.9588 - val_loss: 0.2552 - val_accuracy: 0.9273\n",
      "Epoch 879/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1617 - accuracy: 0.9630 - val_loss: 0.2555 - val_accuracy: 0.9091\n",
      "Epoch 880/15000\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.1608 - accuracy: 0.9630 - val_loss: 0.2553 - val_accuracy: 0.9091\n",
      "Epoch 881/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9609\n",
      "Yes You are here 0.9609053730964661 0.9272727370262146\n",
      "16/16 [==============================] - 3s 197ms/step - loss: 0.1602 - accuracy: 0.9609 - val_loss: 0.2515 - val_accuracy: 0.9273\n",
      "Epoch 882/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1582 - accuracy: 0.9609 - val_loss: 0.2484 - val_accuracy: 0.9091\n",
      "Epoch 883/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9588\n",
      "Yes You are here 0.9588477611541748 0.9272727370262146\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1586 - accuracy: 0.9588 - val_loss: 0.2458 - val_accuracy: 0.9273\n",
      "Epoch 884/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9609\n",
      "Yes You are here 0.9609053730964661 0.9272727370262146\n",
      "16/16 [==============================] - 2s 157ms/step - loss: 0.1557 - accuracy: 0.9609 - val_loss: 0.2471 - val_accuracy: 0.9273\n",
      "Epoch 885/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9568\n",
      "Yes You are here 0.9567901492118835 0.9272727370262146\n",
      "16/16 [==============================] - 3s 206ms/step - loss: 0.1548 - accuracy: 0.9568 - val_loss: 0.2447 - val_accuracy: 0.9273\n",
      "Epoch 886/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9547\n",
      "Yes You are here 0.9547325372695923 0.9272727370262146\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.1535 - accuracy: 0.9547 - val_loss: 0.2435 - val_accuracy: 0.9273\n",
      "Epoch 887/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9691\n",
      "Yes You are here 0.9691358208656311 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1517 - accuracy: 0.9691 - val_loss: 0.2432 - val_accuracy: 0.9273\n",
      "Epoch 888/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9650\n",
      "Yes You are here 0.9650205969810486 0.9272727370262146\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1515 - accuracy: 0.9650 - val_loss: 0.2416 - val_accuracy: 0.9273\n",
      "Epoch 889/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1516 - accuracy: 0.9650\n",
      "Yes You are here 0.9650205969810486 0.9272727370262146\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1516 - accuracy: 0.9650 - val_loss: 0.2375 - val_accuracy: 0.9273\n",
      "Epoch 890/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9609\n",
      "Yes You are here 0.9609053730964661 0.9272727370262146\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.1487 - accuracy: 0.9609 - val_loss: 0.2355 - val_accuracy: 0.9273\n",
      "Epoch 891/15000\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.1493 - accuracy: 0.9568 - val_loss: 0.2445 - val_accuracy: 0.9091\n",
      "Epoch 892/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9630\n",
      "Yes You are here 0.9629629850387573 0.9272727370262146\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1469 - accuracy: 0.9630 - val_loss: 0.2381 - val_accuracy: 0.9273\n",
      "Epoch 893/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9650\n",
      "Yes You are here 0.9650205969810486 0.9272727370262146\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1453 - accuracy: 0.9650 - val_loss: 0.2334 - val_accuracy: 0.9273\n",
      "Epoch 894/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9609\n",
      "Yes You are here 0.9609053730964661 0.9272727370262146\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.1449 - accuracy: 0.9609 - val_loss: 0.2310 - val_accuracy: 0.9273\n",
      "Epoch 895/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9630\n",
      "Yes You are here 0.9629629850387573 0.9272727370262146\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.1437 - accuracy: 0.9630 - val_loss: 0.2313 - val_accuracy: 0.9273\n",
      "Epoch 896/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9691\n",
      "Yes You are here 0.9691358208656311 0.9272727370262146\n",
      "16/16 [==============================] - 3s 151ms/step - loss: 0.1427 - accuracy: 0.9691 - val_loss: 0.2334 - val_accuracy: 0.9273\n",
      "Epoch 897/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1408 - accuracy: 0.9650\n",
      "Yes You are here 0.9650205969810486 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1408 - accuracy: 0.9650 - val_loss: 0.2399 - val_accuracy: 0.9273\n",
      "Epoch 898/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1398 - accuracy: 0.9671 - val_loss: 0.2591 - val_accuracy: 0.9091\n",
      "Epoch 899/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1386 - accuracy: 0.9650 - val_loss: 0.2915 - val_accuracy: 0.9091\n",
      "Epoch 900/15000\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.1368 - accuracy: 0.9691 - val_loss: 0.3212 - val_accuracy: 0.9091\n",
      "Epoch 901/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1391 - accuracy: 0.9609 - val_loss: 0.2964 - val_accuracy: 0.9091\n",
      "Epoch 902/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1359 - accuracy: 0.9712 - val_loss: 0.3378 - val_accuracy: 0.9091\n",
      "Epoch 903/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1350 - accuracy: 0.9630 - val_loss: 0.4024 - val_accuracy: 0.8909\n",
      "Epoch 904/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1344 - accuracy: 0.9671 - val_loss: 0.4384 - val_accuracy: 0.9091\n",
      "Epoch 905/15000\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.1320 - accuracy: 0.9712 - val_loss: 0.5059 - val_accuracy: 0.9091\n",
      "Epoch 906/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.1498 - accuracy: 0.9568 - val_loss: 0.8774 - val_accuracy: 0.8545\n",
      "Epoch 907/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.2352 - accuracy: 0.9300 - val_loss: 1.2132 - val_accuracy: 0.8909\n",
      "Epoch 908/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.5497 - accuracy: 0.8930 - val_loss: 1.4805 - val_accuracy: 0.7273\n",
      "Epoch 909/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.4461 - accuracy: 0.8724 - val_loss: 1.5259 - val_accuracy: 0.8000\n",
      "Epoch 910/15000\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.3282 - accuracy: 0.9156 - val_loss: 1.1228 - val_accuracy: 0.8727\n",
      "Epoch 911/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.9342\n",
      "Yes You are here 0.9341563582420349 0.9272727370262146\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.4540 - accuracy: 0.9342 - val_loss: 0.6979 - val_accuracy: 0.9273\n",
      "Epoch 912/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.3854 - accuracy: 0.9074 - val_loss: 0.5364 - val_accuracy: 0.9091\n",
      "Epoch 913/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9280\n",
      "Yes You are here 0.9279835224151611 0.9272727370262146\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2466 - accuracy: 0.9280 - val_loss: 0.5534 - val_accuracy: 0.9273\n",
      "Epoch 914/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9383\n",
      "Yes You are here 0.9382715821266174 0.9272727370262146\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.2159 - accuracy: 0.9383 - val_loss: 0.5500 - val_accuracy: 0.9273\n",
      "Epoch 915/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9506\n",
      "Yes You are here 0.9506173133850098 0.9272727370262146\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.2019 - accuracy: 0.9506 - val_loss: 0.5785 - val_accuracy: 0.9273\n",
      "Epoch 916/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.9650\n",
      "Yes You are here 0.9650205969810486 0.9272727370262146\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1728 - accuracy: 0.9650 - val_loss: 0.6897 - val_accuracy: 0.9273\n",
      "Epoch 917/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9671\n",
      "Yes You are here 0.9670782089233398 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1652 - accuracy: 0.9671 - val_loss: 0.7527 - val_accuracy: 0.9273\n",
      "Epoch 918/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.1610 - accuracy: 0.9650 - val_loss: 0.7879 - val_accuracy: 0.8909\n",
      "Epoch 919/15000\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.1607 - accuracy: 0.9630 - val_loss: 0.8217 - val_accuracy: 0.9091\n",
      "Epoch 920/15000\n",
      "16/16 [==============================] - 3s 209ms/step - loss: 0.1545 - accuracy: 0.9691 - val_loss: 0.8736 - val_accuracy: 0.9091\n",
      "Epoch 921/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1521 - accuracy: 0.9671 - val_loss: 0.9254 - val_accuracy: 0.9091\n",
      "Epoch 922/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1489 - accuracy: 0.9691 - val_loss: 0.9828 - val_accuracy: 0.9091\n",
      "Epoch 923/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1474 - accuracy: 0.9712 - val_loss: 1.0277 - val_accuracy: 0.9091\n",
      "Epoch 924/15000\n",
      "16/16 [==============================] - 3s 193ms/step - loss: 0.1446 - accuracy: 0.9691 - val_loss: 1.0970 - val_accuracy: 0.9091\n",
      "Epoch 925/15000\n",
      "16/16 [==============================] - 3s 194ms/step - loss: 0.1427 - accuracy: 0.9691 - val_loss: 1.1674 - val_accuracy: 0.9091\n",
      "Epoch 926/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.1407 - accuracy: 0.9691 - val_loss: 1.2179 - val_accuracy: 0.9091\n",
      "Epoch 927/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1398 - accuracy: 0.9712 - val_loss: 1.2753 - val_accuracy: 0.9091\n",
      "Epoch 928/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1378 - accuracy: 0.9691 - val_loss: 1.3686 - val_accuracy: 0.9091\n",
      "Epoch 929/15000\n",
      "16/16 [==============================] - 3s 199ms/step - loss: 0.3154 - accuracy: 0.9691 - val_loss: 0.4333 - val_accuracy: 0.8182\n",
      "Epoch 930/15000\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 1.0225 - accuracy: 0.6811 - val_loss: 0.9373 - val_accuracy: 0.5636\n",
      "Epoch 931/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.7554 - accuracy: 0.7078 - val_loss: 0.6862 - val_accuracy: 0.7455\n",
      "Epoch 932/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5250 - accuracy: 0.8189 - val_loss: 0.5968 - val_accuracy: 0.8182\n",
      "Epoch 933/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4509 - accuracy: 0.8724 - val_loss: 0.5532 - val_accuracy: 0.8545\n",
      "Epoch 934/15000\n",
      "16/16 [==============================] - 3s 201ms/step - loss: 0.4024 - accuracy: 0.8848 - val_loss: 0.5187 - val_accuracy: 0.8364\n",
      "Epoch 935/15000\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.3738 - accuracy: 0.8827 - val_loss: 0.4866 - val_accuracy: 0.8364\n",
      "Epoch 936/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.3481 - accuracy: 0.9053 - val_loss: 0.4613 - val_accuracy: 0.8545\n",
      "Epoch 937/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.3284 - accuracy: 0.9115 - val_loss: 0.4428 - val_accuracy: 0.8727\n",
      "Epoch 938/15000\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.3136 - accuracy: 0.9218 - val_loss: 0.4307 - val_accuracy: 0.8727\n",
      "Epoch 939/15000\n",
      "16/16 [==============================] - 3s 196ms/step - loss: 0.3019 - accuracy: 0.9259 - val_loss: 0.4185 - val_accuracy: 0.8909\n",
      "Epoch 940/15000\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 0.2931 - accuracy: 0.9362 - val_loss: 0.4103 - val_accuracy: 0.8909\n",
      "Epoch 941/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.2857 - accuracy: 0.9321 - val_loss: 0.4021 - val_accuracy: 0.8909\n",
      "Epoch 942/15000\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.2794 - accuracy: 0.9383 - val_loss: 0.3917 - val_accuracy: 0.9091\n",
      "Epoch 943/15000\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.2758 - accuracy: 0.9342 - val_loss: 0.3863 - val_accuracy: 0.9091\n",
      "Epoch 944/15000\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.2692 - accuracy: 0.9342 - val_loss: 0.3839 - val_accuracy: 0.9091\n",
      "Epoch 945/15000\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.2641 - accuracy: 0.9424 - val_loss: 0.3769 - val_accuracy: 0.8909\n",
      "Epoch 946/15000\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.2613 - accuracy: 0.9403 - val_loss: 0.3960 - val_accuracy: 0.8727\n",
      "Epoch 947/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2711 - accuracy: 0.9444 - val_loss: 0.3684 - val_accuracy: 0.9091\n",
      "Epoch 948/15000\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.2675 - accuracy: 0.9444 - val_loss: 0.3755 - val_accuracy: 0.9091\n",
      "Epoch 949/15000\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.2731 - accuracy: 0.9465 - val_loss: 0.3610 - val_accuracy: 0.9091\n",
      "Epoch 950/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.2579 - accuracy: 0.9486 - val_loss: 0.3587 - val_accuracy: 0.9091\n",
      "Epoch 951/15000\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.2425 - accuracy: 0.9547 - val_loss: 0.3542 - val_accuracy: 0.9091\n",
      "Epoch 952/15000\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.2404 - accuracy: 0.9527 - val_loss: 0.3516 - val_accuracy: 0.9091\n",
      "Epoch 953/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2360 - accuracy: 0.9547 - val_loss: 0.3484 - val_accuracy: 0.9091\n",
      "Epoch 954/15000\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.2335 - accuracy: 0.9588 - val_loss: 0.3444 - val_accuracy: 0.9091\n",
      "Epoch 955/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2298 - accuracy: 0.9568 - val_loss: 0.3435 - val_accuracy: 0.9091\n",
      "Epoch 956/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.2275 - accuracy: 0.9547 - val_loss: 0.3424 - val_accuracy: 0.9091\n",
      "Epoch 957/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.2261 - accuracy: 0.9588 - val_loss: 0.3411 - val_accuracy: 0.8909\n",
      "Epoch 958/15000\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.2221 - accuracy: 0.9568 - val_loss: 0.3378 - val_accuracy: 0.9091\n",
      "Epoch 959/15000\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.2195 - accuracy: 0.9588 - val_loss: 0.3357 - val_accuracy: 0.9091\n",
      "Epoch 960/15000\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 0.2176 - accuracy: 0.9568 - val_loss: 0.3350 - val_accuracy: 0.9091\n",
      "Epoch 961/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.2148 - accuracy: 0.9588 - val_loss: 0.3334 - val_accuracy: 0.9091\n",
      "Epoch 962/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2123 - accuracy: 0.9588 - val_loss: 0.3320 - val_accuracy: 0.9091\n",
      "Epoch 963/15000\n",
      "16/16 [==============================] - 3s 199ms/step - loss: 0.2100 - accuracy: 0.9588 - val_loss: 0.3295 - val_accuracy: 0.9091\n",
      "Epoch 964/15000\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 0.2079 - accuracy: 0.9609 - val_loss: 0.3283 - val_accuracy: 0.9091\n",
      "Epoch 965/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.2057 - accuracy: 0.9588 - val_loss: 0.3240 - val_accuracy: 0.9091\n",
      "Epoch 966/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.2036 - accuracy: 0.9630 - val_loss: 0.3227 - val_accuracy: 0.9091\n",
      "Epoch 967/15000\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.2022 - accuracy: 0.9588 - val_loss: 0.3213 - val_accuracy: 0.9091\n",
      "Epoch 968/15000\n",
      "16/16 [==============================] - 3s 209ms/step - loss: 0.1997 - accuracy: 0.9671 - val_loss: 0.3206 - val_accuracy: 0.9091\n",
      "Epoch 969/15000\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.1981 - accuracy: 0.9630 - val_loss: 0.3202 - val_accuracy: 0.9091\n",
      "Epoch 970/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1963 - accuracy: 0.9588 - val_loss: 0.3187 - val_accuracy: 0.9091\n",
      "Epoch 971/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1933 - accuracy: 0.9609 - val_loss: 0.3176 - val_accuracy: 0.9091\n",
      "Epoch 972/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.1921 - accuracy: 0.9609 - val_loss: 0.3159 - val_accuracy: 0.9091\n",
      "Epoch 973/15000\n",
      "16/16 [==============================] - 3s 210ms/step - loss: 0.1901 - accuracy: 0.9630 - val_loss: 0.3113 - val_accuracy: 0.8909\n",
      "Epoch 974/15000\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.1883 - accuracy: 0.9609 - val_loss: 0.3096 - val_accuracy: 0.8909\n",
      "Epoch 975/15000\n",
      "16/16 [==============================] - 2s 157ms/step - loss: 0.1868 - accuracy: 0.9630 - val_loss: 0.3087 - val_accuracy: 0.8909\n",
      "Epoch 976/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1859 - accuracy: 0.9609 - val_loss: 0.3159 - val_accuracy: 0.8727\n",
      "Epoch 977/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1835 - accuracy: 0.9609 - val_loss: 0.3107 - val_accuracy: 0.8909\n",
      "Epoch 978/15000\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.1810 - accuracy: 0.9609 - val_loss: 0.3078 - val_accuracy: 0.8909\n",
      "Epoch 979/15000\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1796 - accuracy: 0.9650 - val_loss: 0.3064 - val_accuracy: 0.8909\n",
      "Epoch 980/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1780 - accuracy: 0.9650 - val_loss: 0.3046 - val_accuracy: 0.8727\n",
      "Epoch 981/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1758 - accuracy: 0.9691 - val_loss: 0.3043 - val_accuracy: 0.8909\n",
      "Epoch 982/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.1742 - accuracy: 0.9630 - val_loss: 0.3009 - val_accuracy: 0.8909\n",
      "Epoch 983/15000\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.1727 - accuracy: 0.9630 - val_loss: 0.3005 - val_accuracy: 0.8909\n",
      "Epoch 984/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1717 - accuracy: 0.9691 - val_loss: 0.2975 - val_accuracy: 0.8909\n",
      "Epoch 985/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.1704 - accuracy: 0.9691 - val_loss: 0.2989 - val_accuracy: 0.8909\n",
      "Epoch 986/15000\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.1685 - accuracy: 0.9691 - val_loss: 0.2887 - val_accuracy: 0.9091\n",
      "Epoch 987/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1761 - accuracy: 0.9650 - val_loss: 0.2888 - val_accuracy: 0.9091\n",
      "Epoch 988/15000\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.2501 - accuracy: 0.9342 - val_loss: 0.3730 - val_accuracy: 0.8909\n",
      "Epoch 989/15000\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.2094 - accuracy: 0.9568 - val_loss: 0.2658 - val_accuracy: 0.9091\n",
      "Epoch 990/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.1802 - accuracy: 0.9650 - val_loss: 0.2774 - val_accuracy: 0.9091\n",
      "Epoch 991/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1736 - accuracy: 0.9671 - val_loss: 0.2848 - val_accuracy: 0.9091\n",
      "Epoch 992/15000\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.1699 - accuracy: 0.9691 - val_loss: 0.2868 - val_accuracy: 0.9091\n",
      "Epoch 993/15000\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.1678 - accuracy: 0.9691 - val_loss: 0.2852 - val_accuracy: 0.9091\n",
      "Epoch 994/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1660 - accuracy: 0.9691 - val_loss: 0.2815 - val_accuracy: 0.9091\n",
      "Epoch 995/15000\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1642 - accuracy: 0.9691 - val_loss: 0.2822 - val_accuracy: 0.9091\n",
      "Epoch 996/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1629 - accuracy: 0.9691 - val_loss: 0.2837 - val_accuracy: 0.9091\n",
      "Epoch 997/15000\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 0.1631 - accuracy: 0.9691 - val_loss: 0.2822 - val_accuracy: 0.9091\n",
      "Epoch 998/15000\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 0.1605 - accuracy: 0.9691 - val_loss: 0.2796 - val_accuracy: 0.9091\n",
      "Epoch 999/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.1577 - accuracy: 0.9691 - val_loss: 0.2781 - val_accuracy: 0.9091\n",
      "Epoch 1000/15000\n",
      "16/16 [==============================] - 2s 158ms/step - loss: 0.1556 - accuracy: 0.9691 - val_loss: 0.2774 - val_accuracy: 0.9091\n",
      "Epoch 1001/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1538 - accuracy: 0.9691 - val_loss: 0.2747 - val_accuracy: 0.9091\n",
      "Epoch 1002/15000\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.1524 - accuracy: 0.9691 - val_loss: 0.2717 - val_accuracy: 0.9091\n",
      "Epoch 1003/15000\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.1509 - accuracy: 0.9691 - val_loss: 0.2671 - val_accuracy: 0.9091\n",
      "Epoch 1004/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1498 - accuracy: 0.9691 - val_loss: 0.2691 - val_accuracy: 0.9091\n",
      "Epoch 1005/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1478 - accuracy: 0.9691 - val_loss: 0.2683 - val_accuracy: 0.9091\n",
      "Epoch 1006/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1468 - accuracy: 0.9712 - val_loss: 0.2657 - val_accuracy: 0.9091\n",
      "Epoch 1007/15000\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.1456 - accuracy: 0.9733 - val_loss: 0.2657 - val_accuracy: 0.9091\n",
      "Epoch 1008/15000\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.1438 - accuracy: 0.9733 - val_loss: 0.2662 - val_accuracy: 0.9091\n",
      "Epoch 1009/15000\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.1424 - accuracy: 0.9733 - val_loss: 0.2654 - val_accuracy: 0.9091\n",
      "Epoch 1010/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1414 - accuracy: 0.9733 - val_loss: 0.2634 - val_accuracy: 0.9091\n",
      "Epoch 1011/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1394 - accuracy: 0.9691 - val_loss: 0.2632 - val_accuracy: 0.8909\n",
      "Epoch 1012/15000\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.1384 - accuracy: 0.9712 - val_loss: 0.2642 - val_accuracy: 0.9091\n",
      "Epoch 1013/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1374 - accuracy: 0.9733 - val_loss: 0.2597 - val_accuracy: 0.9091\n",
      "Epoch 1014/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1365 - accuracy: 0.9733 - val_loss: 0.2572 - val_accuracy: 0.9091\n",
      "Epoch 1015/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9753\n",
      "Yes You are here 0.9753086566925049 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1358 - accuracy: 0.9753 - val_loss: 0.2601 - val_accuracy: 0.9273\n",
      "Epoch 1016/15000\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.1334 - accuracy: 0.9753 - val_loss: 0.2600 - val_accuracy: 0.9091\n",
      "Epoch 1017/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9753\n",
      "Yes You are here 0.9753086566925049 0.9272727370262146\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.1335 - accuracy: 0.9753 - val_loss: 0.2479 - val_accuracy: 0.9273\n",
      "Epoch 1018/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9753\n",
      "Yes You are here 0.9753086566925049 0.9272727370262146\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.1322 - accuracy: 0.9753 - val_loss: 0.2510 - val_accuracy: 0.9273\n",
      "Epoch 1019/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9712\n",
      "Yes You are here 0.9711934328079224 0.9272727370262146\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.1311 - accuracy: 0.9712 - val_loss: 0.2605 - val_accuracy: 0.9273\n",
      "Epoch 1020/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9712\n",
      "Yes You are here 0.9711934328079224 0.9272727370262146\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1299 - accuracy: 0.9712 - val_loss: 0.2530 - val_accuracy: 0.9273\n",
      "Epoch 1021/15000\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.1552 - accuracy: 0.9712 - val_loss: 0.2554 - val_accuracy: 0.9091\n",
      "Epoch 1022/15000\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.1539 - accuracy: 0.9712 - val_loss: 0.2663 - val_accuracy: 0.9091\n",
      "Epoch 1023/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1946 - accuracy: 0.9691 - val_loss: 0.2619 - val_accuracy: 0.9091\n",
      "Epoch 1024/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1458 - accuracy: 0.9671 - val_loss: 0.2497 - val_accuracy: 0.8909\n",
      "Epoch 1025/15000\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.1366 - accuracy: 0.9691 - val_loss: 0.2500 - val_accuracy: 0.8909\n",
      "Epoch 1026/15000\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.1427 - accuracy: 0.9671 - val_loss: 0.2469 - val_accuracy: 0.9091\n",
      "Epoch 1027/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9712\n",
      "Yes You are here 0.9711934328079224 0.9272727370262146\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.1363 - accuracy: 0.9712 - val_loss: 0.2407 - val_accuracy: 0.9273\n",
      "Epoch 1028/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.1289 - accuracy: 0.9733 - val_loss: 0.2440 - val_accuracy: 0.8909\n",
      "Epoch 1029/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1257 - accuracy: 0.9691 - val_loss: 0.2447 - val_accuracy: 0.9091\n",
      "Epoch 1030/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1234 - accuracy: 0.9712 - val_loss: 0.2412 - val_accuracy: 0.9091\n",
      "Epoch 1031/15000\n",
      "16/16 [==============================] - 3s 197ms/step - loss: 0.1241 - accuracy: 0.9753 - val_loss: 0.2424 - val_accuracy: 0.9091\n",
      "Epoch 1032/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9753\n",
      "Yes You are here 0.9753086566925049 0.9272727370262146\n",
      "16/16 [==============================] - 3s 197ms/step - loss: 0.1202 - accuracy: 0.9753 - val_loss: 0.2422 - val_accuracy: 0.9273\n",
      "Epoch 1033/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1182 - accuracy: 0.9712 - val_loss: 0.2413 - val_accuracy: 0.9091\n",
      "Epoch 1034/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9712\n",
      "Yes You are here 0.9711934328079224 0.9272727370262146\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.1170 - accuracy: 0.9712 - val_loss: 0.2375 - val_accuracy: 0.9273\n",
      "Epoch 1035/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9733\n",
      "Yes You are here 0.9732510447502136 0.9272727370262146\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1153 - accuracy: 0.9733 - val_loss: 0.2312 - val_accuracy: 0.9273\n",
      "Epoch 1036/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9753\n",
      "Yes You are here 0.9753086566925049 0.9272727370262146\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.1144 - accuracy: 0.9753 - val_loss: 0.2341 - val_accuracy: 0.9273\n",
      "Epoch 1037/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9753\n",
      "Yes You are here 0.9753086566925049 0.9272727370262146\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 0.1132 - accuracy: 0.9753 - val_loss: 0.2334 - val_accuracy: 0.9273\n",
      "Epoch 1038/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9753\n",
      "Yes You are here 0.9753086566925049 0.9272727370262146\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.1113 - accuracy: 0.9753 - val_loss: 0.2320 - val_accuracy: 0.9273\n",
      "Epoch 1039/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9733\n",
      "Yes You are here 0.9732510447502136 0.9272727370262146\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.1103 - accuracy: 0.9733 - val_loss: 0.2309 - val_accuracy: 0.9273\n",
      "Epoch 1040/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.1094 - accuracy: 0.9794 - val_loss: 0.2280 - val_accuracy: 0.9273\n",
      "Epoch 1041/15000\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.1079 - accuracy: 0.9794 - val_loss: 0.2281 - val_accuracy: 0.9091\n",
      "Epoch 1042/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1070 - accuracy: 0.9815 - val_loss: 0.2302 - val_accuracy: 0.9091\n",
      "Epoch 1043/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9815\n",
      "Yes You are here 0.9814814925193787 0.9272727370262146\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.1064 - accuracy: 0.9815 - val_loss: 0.2270 - val_accuracy: 0.9273\n",
      "Epoch 1044/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1044 - accuracy: 0.9794 - val_loss: 0.2251 - val_accuracy: 0.9273\n",
      "Epoch 1045/15000\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.1032 - accuracy: 0.9815 - val_loss: 0.2247 - val_accuracy: 0.9091\n",
      "Epoch 1046/15000\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.1091 - accuracy: 0.9794 - val_loss: 1.5496 - val_accuracy: 0.9091\n",
      "Epoch 1047/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.9444\n",
      "Yes You are here 0.9444444179534912 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.4262 - accuracy: 0.9444 - val_loss: 0.2078 - val_accuracy: 0.9273\n",
      "Epoch 1048/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2396 - accuracy: 0.9588 - val_loss: 0.7697 - val_accuracy: 0.9091\n",
      "Epoch 1049/15000\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 5.8991 - accuracy: 0.9156 - val_loss: 9.2153 - val_accuracy: 0.7455\n",
      "Epoch 1050/15000\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 2.0135 - accuracy: 0.8230 - val_loss: 0.8192 - val_accuracy: 0.8545\n",
      "Epoch 1051/15000\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 0.3314 - accuracy: 0.9074 - val_loss: 0.4946 - val_accuracy: 0.8727\n",
      "Epoch 1052/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2691 - accuracy: 0.9383 - val_loss: 0.4617 - val_accuracy: 0.8727\n",
      "Epoch 1053/15000\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.2408 - accuracy: 0.9444 - val_loss: 0.4389 - val_accuracy: 0.8727\n",
      "Epoch 1054/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.2243 - accuracy: 0.9506 - val_loss: 0.4093 - val_accuracy: 0.8909\n",
      "Epoch 1055/15000\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.2141 - accuracy: 0.9527 - val_loss: 0.3940 - val_accuracy: 0.8727\n",
      "Epoch 1056/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2056 - accuracy: 0.9568 - val_loss: 0.3808 - val_accuracy: 0.8909\n",
      "Epoch 1057/15000\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.2004 - accuracy: 0.9588 - val_loss: 0.3687 - val_accuracy: 0.8909\n",
      "Epoch 1058/15000\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.1946 - accuracy: 0.9588 - val_loss: 0.3623 - val_accuracy: 0.9091\n",
      "Epoch 1059/15000\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1905 - accuracy: 0.9630 - val_loss: 0.3558 - val_accuracy: 0.9091\n",
      "Epoch 1060/15000\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.1873 - accuracy: 0.9547 - val_loss: 0.3471 - val_accuracy: 0.9091\n",
      "Epoch 1061/15000\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1834 - accuracy: 0.9609 - val_loss: 0.3455 - val_accuracy: 0.9091\n",
      "Epoch 1062/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1791 - accuracy: 0.9588 - val_loss: 0.3419 - val_accuracy: 0.9091\n",
      "Epoch 1063/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1770 - accuracy: 0.9609 - val_loss: 0.3392 - val_accuracy: 0.9091\n",
      "Epoch 1064/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1735 - accuracy: 0.9630 - val_loss: 0.3340 - val_accuracy: 0.9091\n",
      "Epoch 1065/15000\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.1704 - accuracy: 0.9630 - val_loss: 0.3322 - val_accuracy: 0.9091\n",
      "Epoch 1066/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1673 - accuracy: 0.9650 - val_loss: 0.3282 - val_accuracy: 0.9091\n",
      "Epoch 1067/15000\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1648 - accuracy: 0.9671 - val_loss: 0.3237 - val_accuracy: 0.9091\n",
      "Epoch 1068/15000\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1624 - accuracy: 0.9650 - val_loss: 0.3219 - val_accuracy: 0.9091\n",
      "Epoch 1069/15000\n",
      "16/16 [==============================] - 3s 211ms/step - loss: 0.1600 - accuracy: 0.9650 - val_loss: 0.3199 - val_accuracy: 0.9091\n",
      "Epoch 1070/15000\n",
      "16/16 [==============================] - 3s 203ms/step - loss: 0.1578 - accuracy: 0.9650 - val_loss: 0.3168 - val_accuracy: 0.9091\n",
      "Epoch 1071/15000\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1561 - accuracy: 0.9671 - val_loss: 0.3136 - val_accuracy: 0.9091\n",
      "Epoch 1072/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1538 - accuracy: 0.9671 - val_loss: 0.3103 - val_accuracy: 0.9091\n",
      "Epoch 1073/15000\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.1518 - accuracy: 0.9671 - val_loss: 0.3073 - val_accuracy: 0.9091\n",
      "Epoch 1074/15000\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.1495 - accuracy: 0.9671 - val_loss: 0.3063 - val_accuracy: 0.9091\n",
      "Epoch 1075/15000\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.1475 - accuracy: 0.9691 - val_loss: 0.3045 - val_accuracy: 0.9091\n",
      "Epoch 1076/15000\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1459 - accuracy: 0.9671 - val_loss: 0.3008 - val_accuracy: 0.9091\n",
      "Epoch 1077/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9671\n",
      "Yes You are here 0.9670782089233398 0.9272727370262146\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.1446 - accuracy: 0.9671 - val_loss: 0.2966 - val_accuracy: 0.9273\n",
      "Epoch 1078/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9712\n",
      "Yes You are here 0.9711934328079224 0.9272727370262146\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1420 - accuracy: 0.9712 - val_loss: 0.2974 - val_accuracy: 0.9273\n",
      "Epoch 1079/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9712\n",
      "Yes You are here 0.9711934328079224 0.9272727370262146\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.1405 - accuracy: 0.9712 - val_loss: 0.2957 - val_accuracy: 0.9273\n",
      "Epoch 1080/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9712\n",
      "Yes You are here 0.9711934328079224 0.9272727370262146\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.1388 - accuracy: 0.9712 - val_loss: 0.2932 - val_accuracy: 0.9273\n",
      "Epoch 1081/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1365 - accuracy: 0.9774 - val_loss: 0.2927 - val_accuracy: 0.9273\n",
      "Epoch 1082/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1354 - accuracy: 0.9774 - val_loss: 0.2926 - val_accuracy: 0.9273\n",
      "Epoch 1083/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1338 - accuracy: 0.9774 - val_loss: 0.2889 - val_accuracy: 0.9273\n",
      "Epoch 1084/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.1320 - accuracy: 0.9774 - val_loss: 0.2868 - val_accuracy: 0.9273\n",
      "Epoch 1085/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.1307 - accuracy: 0.9774 - val_loss: 0.2841 - val_accuracy: 0.9273\n",
      "Epoch 1086/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 157ms/step - loss: 0.1286 - accuracy: 0.9774 - val_loss: 0.2831 - val_accuracy: 0.9273\n",
      "Epoch 1087/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.1271 - accuracy: 0.9794 - val_loss: 0.2816 - val_accuracy: 0.9273\n",
      "Epoch 1088/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.1257 - accuracy: 0.9794 - val_loss: 0.2812 - val_accuracy: 0.9273\n",
      "Epoch 1089/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 196ms/step - loss: 0.1243 - accuracy: 0.9794 - val_loss: 0.2787 - val_accuracy: 0.9273\n",
      "Epoch 1090/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.1230 - accuracy: 0.9794 - val_loss: 0.2747 - val_accuracy: 0.9273\n",
      "Epoch 1091/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.1216 - accuracy: 0.9774 - val_loss: 0.2746 - val_accuracy: 0.9273\n",
      "Epoch 1092/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.1201 - accuracy: 0.9794 - val_loss: 0.2700 - val_accuracy: 0.9273\n",
      "Epoch 1093/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.1187 - accuracy: 0.9794 - val_loss: 0.2655 - val_accuracy: 0.9273\n",
      "Epoch 1094/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 0.1175 - accuracy: 0.9774 - val_loss: 0.2648 - val_accuracy: 0.9273\n",
      "Epoch 1095/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1159 - accuracy: 0.9794 - val_loss: 0.2665 - val_accuracy: 0.9273\n",
      "Epoch 1096/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.1149 - accuracy: 0.9794 - val_loss: 0.2658 - val_accuracy: 0.9273\n",
      "Epoch 1097/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.1138 - accuracy: 0.9774 - val_loss: 0.2625 - val_accuracy: 0.9273\n",
      "Epoch 1098/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.1125 - accuracy: 0.9774 - val_loss: 0.2656 - val_accuracy: 0.9273\n",
      "Epoch 1099/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1110 - accuracy: 0.9774 - val_loss: 0.2624 - val_accuracy: 0.9273\n",
      "Epoch 1100/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1099 - accuracy: 0.9774 - val_loss: 0.2560 - val_accuracy: 0.9273\n",
      "Epoch 1101/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.1093 - accuracy: 0.9774 - val_loss: 0.2578 - val_accuracy: 0.9273\n",
      "Epoch 1102/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 198ms/step - loss: 0.1076 - accuracy: 0.9774 - val_loss: 0.2558 - val_accuracy: 0.9273\n",
      "Epoch 1103/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 205ms/step - loss: 0.1062 - accuracy: 0.9794 - val_loss: 0.2554 - val_accuracy: 0.9273\n",
      "Epoch 1104/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.1054 - accuracy: 0.9794 - val_loss: 0.2542 - val_accuracy: 0.9273\n",
      "Epoch 1105/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.1044 - accuracy: 0.9794 - val_loss: 0.2555 - val_accuracy: 0.9273\n",
      "Epoch 1106/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1031 - accuracy: 0.9794 - val_loss: 0.2514 - val_accuracy: 0.9273\n",
      "Epoch 1107/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.1022 - accuracy: 0.9774 - val_loss: 0.2532 - val_accuracy: 0.9273\n",
      "Epoch 1108/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.1015 - accuracy: 0.9774 - val_loss: 0.2510 - val_accuracy: 0.9273\n",
      "Epoch 1109/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 157ms/step - loss: 0.1003 - accuracy: 0.9774 - val_loss: 0.2509 - val_accuracy: 0.9273\n",
      "Epoch 1110/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.0991 - accuracy: 0.9774 - val_loss: 0.2496 - val_accuracy: 0.9273\n",
      "Epoch 1111/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0981 - accuracy: 0.9774 - val_loss: 0.2459 - val_accuracy: 0.9273\n",
      "Epoch 1112/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0971 - accuracy: 0.9774 - val_loss: 0.2462 - val_accuracy: 0.9273\n",
      "Epoch 1113/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0963 - accuracy: 0.9774 - val_loss: 0.2452 - val_accuracy: 0.9273\n",
      "Epoch 1114/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.0953 - accuracy: 0.9794 - val_loss: 0.2453 - val_accuracy: 0.9273\n",
      "Epoch 1115/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.0940 - accuracy: 0.9774 - val_loss: 0.2432 - val_accuracy: 0.9273\n",
      "Epoch 1116/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0936 - accuracy: 0.9794 - val_loss: 0.2391 - val_accuracy: 0.9273\n",
      "Epoch 1117/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0927 - accuracy: 0.9794 - val_loss: 0.2405 - val_accuracy: 0.9273\n",
      "Epoch 1118/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0913 - accuracy: 0.9794 - val_loss: 0.2396 - val_accuracy: 0.9273\n",
      "Epoch 1119/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.0910 - accuracy: 0.9794 - val_loss: 0.2387 - val_accuracy: 0.9273\n",
      "Epoch 1120/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.0898 - accuracy: 0.9794 - val_loss: 0.2400 - val_accuracy: 0.9273\n",
      "Epoch 1121/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9774\n",
      "Yes You are here 0.9773662686347961 0.9272727370262146\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0896 - accuracy: 0.9774 - val_loss: 0.2379 - val_accuracy: 0.9273\n",
      "Epoch 1122/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0887 - accuracy: 0.9794 - val_loss: 0.2364 - val_accuracy: 0.9273\n",
      "Epoch 1123/15000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9794\n",
      "Yes You are here 0.9794238805770874 0.9272727370262146\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0875 - accuracy: 0.9794 - val_loss: 0.2334 - val_accuracy: 0.9273\n",
      "Epoch 1124/15000\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.0907 - accuracy: 0.9766"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-e58bf86125a1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCustomCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no_clownv0.2.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,batch_size=32, epochs=15000,verbose=1,validation_split=0.10 , validation_batch_size=32,callbacks=[CustomCallback(\"no_clownv0.2.h5\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TesAx6UDgJLT"
   },
   "outputs": [],
   "source": [
    "# Yes You are here 0.9925925731658936 0.9833333492279053\n",
    "# best_model_on_12_sign_(ISA).h5\n",
    "\n",
    "\n",
    "\n",
    "# best_model_on_14_sign_(ISA).h5\n",
    "# Yes You are here 1.0 0.9571428298950195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZj-mvCu9lmz",
    "outputId": "17bf57c5-09f4-4233-fe78-8e521ff1ba0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2,  5, 10, 11,  5,  1,  2,  2,  8,  4, 10,  8,  9,  8,  5,  1,  7,\n",
       "        1,  6,  4,  8,  1,  1,  9,  9,  1,  9,  2,  5, 10, 10,  0, 11,  9,\n",
       "       11,  4, 10,  5,  0, 10, 11,  5,  2,  0,  4,  2,  7,  2,  5,  6,  5,\n",
       "        8,  4,  6,  6, 10,  9,  0,  0,  0,  6,  9,  6,  5, 10, 11,  8,  0,\n",
       "        9,  5,  9,  0,  0,  7,  9,  9,  6, 11,  2, 11,  7,  9, 10,  8,  2,\n",
       "        6, 10, 10,  1,  2,  7,  1,  5,  2,  8, 10,  5,  8,  0,  5,  4,  8,\n",
       "        1,  0,  6,  9,  7,  7,  9,  7,  5,  6,  1,  0,  4,  8,  9,  0,  8,\n",
       "        4,  9, 11,  4,  5,  5,  6,  6,  1,  4,  5,  2, 10, 10,  0, 10, 11,\n",
       "        7,  6, 10,  7,  6,  8,  1,  8,  7,  1, 10,  0,  1,  6, 10,  8,  2,\n",
       "        6,  8,  0, 10,  0,  0,  4,  8,  9,  4,  7,  4,  4,  2, 10, 10,  5,\n",
       "       11,  7,  2,  2,  7,  2,  2,  2,  1,  8,  6,  9,  9,  9, 11,  0, 10,\n",
       "        9,  5,  8,  9, 11,  4,  6,  4,  7,  0,  2,  9,  1,  5, 11, 10,  1,\n",
       "        1,  7,  7,  1,  9, 11,  8,  7,  7,  9,  7, 10, 11, 10,  1,  0,  5,\n",
       "        0,  6,  0,  7,  8,  4, 11,  2,  9,  4, 11,  4, 11,  1,  0,  9,  2,\n",
       "        6,  0,  4,  4,  0,  7,  1,  1, 11,  4,  8,  5,  0,  7,  4,  2,  5,\n",
       "        8,  8,  0,  1,  5, 11,  8,  0,  8,  1,  6,  2,  0, 10, 11,  0,  0,\n",
       "        2,  1,  1,  8,  8, 11,  8,  8,  5,  1,  9,  9,  0,  6,  5,  1,  7,\n",
       "        0,  2,  1,  1,  6,  0,  9,  2,  4,  0, 10,  0,  1,  0, 10,  7, 10,\n",
       "        4,  0,  9,  5,  8,  1,  1,  6,  1, 10,  7,  9, 11,  0,  7,  0,  8,\n",
       "        6,  2,  4,  7,  5, 11,  0,  8,  6,  4,  7,  6,  7,  0,  8,  2,  8,\n",
       "        1,  5,  1,  5,  5,  9,  8,  0,  2,  9,  4,  1, 10,  4,  2, 10,  8,\n",
       "        4, 11,  4,  4, 11,  9,  2, 10,  8,  0,  2,  6,  6,  6,  2,  5,  5,\n",
       "        7,  9,  9,  2, 10, 10,  4,  5, 11,  1,  7,  7, 10,  6,  8, 10,  1,\n",
       "       11,  5,  1,  4,  6,  7,  2, 11,  7, 10, 11,  5, 11,  0,  2,  7, 11,\n",
       "        7,  7,  4,  9, 11,  4,  0,  5,  1, 11, 10,  8, 11,  1,  0,  5, 10,\n",
       "        4,  4,  5,  7,  5,  6,  8,  6,  7,  2, 11,  6, 10,  7,  1,  6, 11,\n",
       "        0,  1,  7, 10,  4,  2,  5,  8,  1,  6,  0,  9, 11,  6, 10,  2,  2,\n",
       "        8,  0,  1,  6,  4,  8,  2,  9,  7,  4,  6,  5,  2,  5,  4,  6,  0,\n",
       "       10,  7,  4, 11, 11, 11,  0, 11,  6, 11,  1,  6,  2, 11,  9,  6,  5,\n",
       "        9,  4])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Y_train[np.argmax(model.predict(X_train),axis=1)  !=np.argmax(Y_train)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EV1-Rth49oi-",
    "outputId": "e4c14c80-1bf8-4052-9b27-8294592e2cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  6,  9,  0,  9,  1, 10,  1,  1,  1,  9, 10,  8, 11])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified = np.argmax(Y_train[np.argmax(model.predict(X_train),axis=1)  != np.argmax(Y_train,axis=1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3rZvJbU_Si_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
